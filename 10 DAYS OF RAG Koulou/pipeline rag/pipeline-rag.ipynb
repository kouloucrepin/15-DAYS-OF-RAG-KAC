{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d4aa8a",
   "metadata": {},
   "source": [
    "## BASE DE DONNEES VECTORIELLE ET EMBEDING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc29c594",
   "metadata": {},
   "source": [
    "## ETAPE 1 CHUNKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1c8db05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def load_and_split_pdf_simple(pdf_path: str):\n",
    "    \"\"\"\n",
    "    Charge tout le PDF en un seul Document, puis le dÃ©coupe en chunks\n",
    "    avec chunk_size=1000 et chunk_overlap=100 (en caractÃ¨res).\n",
    "    \"\"\"\n",
    "    # 1. Extraire tout le texte du PDF\n",
    "    reader = PdfReader(pdf_path)\n",
    "    full_text = \"\"\n",
    "    for i, page in enumerate(reader.pages):\n",
    "        text = page.extract_text() or \"\"\n",
    "        full_text += text + f\"\\n\\n[PAGE {i+1}]\\n\\n\"  # Ajout de marqueur de page\n",
    "\n",
    "    # 2. CrÃ©er un seul Document\n",
    "    doc = Document(page_content=full_text.strip().replace('. .',''), metadata={\"source\": pdf_path})\n",
    "\n",
    "    # 3. DÃ©couper avec RecursiveCharacterTextSplitter (par dÃ©faut : len = caractÃ¨res)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "        separators=['.'],\n",
    "        keep_separator=True  # Garde les sÃ©parateurs pour Ã©viter de couper au milieu d'une phrase\n",
    "    )\n",
    "\n",
    "    # 4. Splitter\n",
    "    chunks = text_splitter.split_documents([doc])\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Utilisation\n",
    "pdf_chunks = load_and_split_pdf_simple(\"embedding_course_by_koulou.pdf\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d58bb5c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Lâ€™INTELLIGENCE ARTIFICIELLE\\nDes fondements thÃ©oriques aux bases de donnÃ©es vectorielles\\nAuteur : KOULOU crepin\\nDÃ©partement de Data Science\\n8 janvier 2026\\n\\n[PAGE 1]\\n\\nRÃ©sumÃ©\\nCe rapport prÃ©sente une analyse approfondie de lâ€™Ã©tat actuel de lâ€™Intelligence Artiï¬cielle.\\nNous explorerons dâ€™abord les fondements mathÃ©matiques de lâ€™apprentissage automatique\\n(Machine Learning) et des rÃ©seaux de neurones profonds (Deep Learning). Une attention\\nparticuliÃ¨re sera portÃ©e aux dÃ©veloppements rÃ©cents concernant les Grands ModÃ¨les de\\nLangage (LLM) et lâ€™infrastructure critique qui les soutient, notamment les bases de don-\\nnÃ©es vectorielles (FAISS, ChromaDB, Qdrant) et les mÃ©canismes de similaritÃ©. Enï¬n, nous\\naborderons les dÃ©ï¬s Ã©thiques et les perspectives dâ€™avenir.\\n\\n[PAGE 2]\\n\\nTable des matiÃ¨res\\n1 Introduction 2\\n1.1 Contexte Historique                . 2\\n1.2 DÃ©ï¬nitions et Typologie               2\\n2 Fondements MathÃ©matiques du Machine Learning 4\\n2.1 Lâ€™Apprentissage SupervisÃ©              . 4\\n2', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='.1 Lâ€™Apprentissage SupervisÃ©              . 4\\n2.2 RÃ©seaux de Neurones et RÃ©tropropagation          4\\n3 Deep Learning et Architecture Moderne 6\\n3.1 Les RÃ©seaux de Neurones Convolutifs (CNN)         . 6\\n3.2 Les Transformers et le MÃ©canisme dâ€™Attention         6\\n4 Infrastructure de lâ€™IA : Embeddings et Bases Vectorielles 8\\n4.1 Le Concept dâ€™Embedding (Plongement Lexical)        . 8\\n4.2 Mesure de la SimilaritÃ©               . 8\\n4.3 Les Bases de DonnÃ©es Vectorielles            . 8\\n4.3.1 Comparatif Technique             . 9\\n4.3.2 Exemple dâ€™implÃ©mentation (Python)         . 9\\n5 DÃ©ï¬s Ã‰thiques et Avenir 11\\n5.1 Biais Algorithmiques et Ã‰quitÃ©             . 11\\n5.2 RÃ©gulation (AI Act)                11\\n6 Conclusion 12\\n\\n[PAGE 3]\\n\\nChapitre 1\\nIntroduction\\nLâ€™Intelligence Artiï¬cielle (IA) a cessÃ© dâ€™Ãªtre un concept de science-ï¬ction pour devenir\\nune force motrice de lâ€™Ã©conomie mondiale.\\n1.1 Contexte Historique\\nDepuis le test de Turing jusquâ€™Ã  lâ€™avÃ¨nement de ChatGPT, lâ€™Ã©volution a Ã©tÃ© exponen-\\ntielle', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Les premiers systÃ¨mes experts ont laissÃ© place aux rÃ©seaux de neurones. Lâ€™intelli-\\ngence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©ment notre\\nsociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique permettent aux ordinateurs dâ€™ap-\\nprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque tÃ¢che. Dans\\nce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vectorielles jouent\\nun rÃ´le crucial pour traiter des informations non structurÃ©es. Ce paragraphe sert de rem-\\nplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©cier la mise\\nen page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques rÃ©centes, notamment\\nles architectures Transformers, ont ouvert la voie aux modÃ¨les de langage massifs (LLM).\\nLâ€™intelligence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©-\\nment notre sociÃ©tÃ©', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Les algorithmes dâ€™apprentissage automatique permettent aux ordina-\\nteurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque\\ntÃ¢che. Dans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vecto-\\nrielles jouent un rÃ´le crucial pour traiter des informations non structurÃ©es. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©-\\ncier la mise en page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques rÃ©centes,\\nnotamment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage\\nmassifs (LLM).\\n1.2 DÃ©ï¬nitions et Typologie\\nIl convient de distinguer lâ€™IA faible (ANI), spÃ©cialisÃ©e dans une tÃ¢che, de lâ€™IA forte\\n(AGI), hypothÃ©tique et capable de gÃ©nÃ©ralisation humaine. Lâ€™intelligence artiï¬cielle est un\\ndomaine en pleine expansion qui transforme profondÃ©ment notre sociÃ©tÃ©', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Les algorithmes\\ndâ€™apprentissage automatique permettent aux ordinateurs dâ€™apprendre Ã  partir de donnÃ©es\\nsans Ãªtre explicitement programmÃ©s pour chaque tÃ¢che. Dans ce contexte, les rÃ©seaux de\\nneuronesprofondsetlesbasesdedonnÃ©esvectoriellesjouentunrÃ´lecrucialpourtraiterdes\\n2\\n\\n[PAGE 4]\\n\\nÃ‰tat de lâ€™art Rapport IA 2024\\ninformations non structurÃ©es. Ce paragraphe sert de remplissage pour simuler le volume\\nde texte du rapport ï¬nal, permettant dâ€™apprÃ©cier la mise en page sans Ãªtre distrait par\\ndu latin. Les avancÃ©es technologiques rÃ©centes, notamment les architectures Transformers,\\nont ouvert la voie aux modÃ¨les de langage massifs (LLM).\\n3\\n\\n[PAGE 5]\\n\\nChapitre 2\\nFondements MathÃ©matiques du\\nMachine Learning\\nLâ€™IA repose avant tout sur lâ€™optimisation mathÃ©matique et les statistiques avancÃ©es.\\n2.1 Lâ€™Apprentissage SupervisÃ©\\nDans un cadre de rÃ©gression ou de classiï¬cation, nous cherchons Ã  minimiser une\\nfonction de coÃ»t (Loss Function).\\nSoit un jeu de donnÃ©esD = {(xi,yi)}n\\ni=1', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='.\\nSoit un jeu de donnÃ©esD = {(xi,yi)}n\\ni=1. Pour un modÃ¨le linÃ©airef(x) = wT x+ b,\\nnous cherchons Ã  minimiser lâ€™erreur quadratique moyenne (MSE) :\\nJ(w,b) = 1\\nn\\nnâˆ‘\\ni=1\\n(yi âˆ’(wT xi + b))2 (2.1)\\nLâ€™algorithme de descente de gradient permet de mettre Ã  jour les poidsw de maniÃ¨re\\nitÃ©rative :\\nwâ†wâˆ’Î±âˆ‡wJ(w,b) (2.2)\\noÃ¹ Î± reprÃ©sente le taux dâ€™apprentissage (learning rate), un hyperparamÃ¨tre crucial.\\n2.2 RÃ©seaux de Neurones et RÃ©tropropagation\\nUn neurone artiï¬ciel applique une fonction dâ€™activation non linÃ©aireÏƒ (comme ReLU\\nou Sigmoid) :\\na[l] = Ïƒ(W[l]a[lâˆ’1] + b[l]) (2.3)\\nLâ€™intelligence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©-\\nment notre sociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique permettent aux ordina-\\nteurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque\\ntÃ¢che. Dans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vecto-\\nrielles jouent un rÃ´le crucial pour traiter des informations non structurÃ©es', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©-\\ncier la mise en page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques rÃ©centes,\\n4\\n\\n[PAGE 6]\\n\\nÃ‰tat de lâ€™art Rapport IA 2024\\nnotamment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage\\nmassifs (LLM).\\nLâ€™intelligence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©-\\nment notre sociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique permettent aux ordina-\\nteurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque\\ntÃ¢che. Dans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vecto-\\nrielles jouent un rÃ´le crucial pour traiter des informations non structurÃ©es. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©-\\ncier la mise en page sans Ãªtre distrait par du latin', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Les avancÃ©es technologiques rÃ©centes,\\nnotamment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage\\nmassifs (LLM).\\nLâ€™intelligence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©-\\nment notre sociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique permettent aux ordina-\\nteurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque\\ntÃ¢che. Dans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vecto-\\nrielles jouent un rÃ´le crucial pour traiter des informations non structurÃ©es. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©-\\ncier la mise en page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques rÃ©centes,\\nnotamment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage\\nmassifs (LLM).\\nLâ€™intelligence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©-\\nment notre sociÃ©tÃ©', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Les algorithmes dâ€™apprentissage automatique permettent aux ordina-\\nteurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque\\ntÃ¢che. Dans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vecto-\\nrielles jouent un rÃ´le crucial pour traiter des informations non structurÃ©es. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©-\\ncier la mise en page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques rÃ©centes,\\nnotamment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage\\nmassifs (LLM).\\n5\\n\\n[PAGE 7]\\n\\nChapitre 3\\nDeep Learning et Architecture Moderne\\n3.1 Les RÃ©seaux de Neurones Convolutifs (CNN)\\nUtilisÃ©s principalement en vision par ordinateur pour la reconnaissance dâ€™images. Lâ€™in-\\ntelligence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©ment\\nnotre sociÃ©tÃ©', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Les algorithmes dâ€™apprentissage automatique permettent aux ordinateurs\\ndâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque tÃ¢che.\\nDans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vectorielles\\njouent un rÃ´le crucial pour traiter des informations non structurÃ©es. Ce paragraphe sert\\nde remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©cier la\\nmise en page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques rÃ©centes, notam-\\nment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage massifs\\n(LLM).\\n3.2 Les Transformers et le MÃ©canisme dâ€™Attention\\nLâ€™innovation majeure de lâ€™article fondateur de 2017 a rÃ©volutionnÃ© le traitement du\\nlangage naturel (NLP). Lâ€™attention se calcule ainsi :\\nAttention(Q,K,V ) =softmax\\n(QKT\\nâˆšdk\\n)\\nV (3.1)\\nCette architecture permet de parallÃ©liser les calculs, contrairement aux rÃ©seaux rÃ©cur-\\nrents (RNN) prÃ©cÃ©dents', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Lâ€™intelligence artiï¬cielle est un domaine en pleine expansion\\nqui transforme profondÃ©ment notre sociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique\\npermettent aux ordinateurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement pro-\\ngrammÃ©s pour chaque tÃ¢che. Dans ce contexte, les rÃ©seaux de neurones profonds et les\\nbases de donnÃ©es vectorielles jouent un rÃ´le crucial pour traiter des informations non\\nstructurÃ©es. Ce paragraphe sert de remplissage pour simuler le volume de texte du rap-\\nport ï¬nal, permettant dâ€™apprÃ©cier la mise en page sans Ãªtre distrait par du latin. Les\\navancÃ©es technologiques rÃ©centes, notamment les architectures Transformers, ont ouvert\\nla voie aux modÃ¨les de langage massifs (LLM).\\nLâ€™intelligence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©-\\nment notre sociÃ©tÃ©', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Les algorithmes dâ€™apprentissage automatique permettent aux ordina-\\nteurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque\\n6\\n\\n[PAGE 8]\\n\\nÃ‰tat de lâ€™art Rapport IA 2024\\ntÃ¢che. Dans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vecto-\\nrielles jouent un rÃ´le crucial pour traiter des informations non structurÃ©es. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©-\\ncier la mise en page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques rÃ©centes,\\nnotamment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage\\nmassifs (LLM).\\nLâ€™intelligence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©-\\nment notre sociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique permettent aux ordina-\\nteurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque\\ntÃ¢che', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Dans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vecto-\\nrielles jouent un rÃ´le crucial pour traiter des informations non structurÃ©es. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©-\\ncier la mise en page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques rÃ©centes,\\nnotamment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage\\nmassifs (LLM).\\nLâ€™intelligence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©-\\nment notre sociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique permettent aux ordina-\\nteurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque\\ntÃ¢che. Dans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vecto-\\nrielles jouent un rÃ´le crucial pour traiter des informations non structurÃ©es', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©-\\ncier la mise en page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques rÃ©centes,\\nnotamment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage\\nmassifs (LLM).\\n7\\n\\n[PAGE 9]\\n\\nChapitre 4\\nInfrastructure de lâ€™IA : Embeddings et\\nBases Vectorielles\\nCâ€™est ici que rÃ©side le cÅ“ur des systÃ¨mes RAG (GÃ©nÃ©ration AugmentÃ©e par la RÃ©cu-\\npÃ©ration) modernes.\\n4.1 Le Concept dâ€™Embedding (Plongement Lexical)\\nLes embeddings transforment des donnÃ©es non structurÃ©es (texte, image, son) en vec-\\nteurs denses de dimensiond. Lâ€™intelligence artiï¬cielle est un domaine en pleine expansion\\nqui transforme profondÃ©ment notre sociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique\\npermettent aux ordinateurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement pro-\\ngrammÃ©s pour chaque tÃ¢che', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Dans ce contexte, les rÃ©seaux de neurones profonds et les\\nbases de donnÃ©es vectorielles jouent un rÃ´le crucial pour traiter des informations non\\nstructurÃ©es. Ce paragraphe sert de remplissage pour simuler le volume de texte du rap-\\nport ï¬nal, permettant dâ€™apprÃ©cier la mise en page sans Ãªtre distrait par du latin. Les\\navancÃ©es technologiques rÃ©centes, notamment les architectures Transformers, ont ouvert\\nla voie aux modÃ¨les de langage massifs (LLM).\\n4.2 Mesure de la SimilaritÃ©\\nComme vu prÃ©cÃ©demment, la similaritÃ© cosinus est la mÃ©trique standard pour Ã©valuer\\nla proximitÃ© sÃ©mantique :\\nsim(A,B) = cos(Î¸) = AÂ·B\\nâˆ¥Aâˆ¥âˆ¥Bâˆ¥=\\nâˆ‘n\\ni=1 AiBiâˆšâˆ‘n\\ni=1 A2\\ni\\nâˆšâˆ‘n\\ni=1 B2\\ni\\n(4.1)\\n4.3 Les Bases de DonnÃ©es Vectorielles\\nPour gÃ©rer des millions de vecteurs, des solutions spÃ©cialisÃ©es sont nÃ©cessaires. Les\\nbases de donnÃ©es relationnelles classiques (SQL) ne sont pas optimisÃ©es pour ce type de\\ncalcul.\\n8\\n\\n[PAGE 10]\\n\\nÃ‰tat de lâ€™art Rapport IA 2024\\n4.3', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='.\\n8\\n\\n[PAGE 10]\\n\\nÃ‰tat de lâ€™art Rapport IA 2024\\n4.3.1 Comparatif Technique\\nâ€” FAISS (Meta): BibliothÃ¨que bas niveau, optimisÃ©e pour la recherche par simila-\\nritÃ© dense et le clustering. Utilise lâ€™indexation IVF.\\nâ€” ChromaDB : Solution open-source conviviale pour les dÃ©veloppeurs, idÃ©ale pour\\nles applications LLM lÃ©gÃ¨res et le prototypage.\\nâ€” Pinecone / Qdrant: Solutions gÃ©rÃ©es (SaaS) ou robustes (Rust) pour la produc-\\ntion Ã  grande Ã©chelle.\\n4.3.2 Exemple dâ€™implÃ©mentation (Python)\\nVoici comment on initialise une recherche simple avec la librairie ChromaDB :\\n1 import chromadb\\n2\\n3 # Initialisation du client (en mÃ© moire pour lâ€™ exemple )\\n4 client = chromadb . Client ()\\n5 collection = client . create_collection (\" mon_dataset_ia \")\\n6\\n7 # Ajout de documents ( texte brut + mÃ© tadonn Ã©es)\\n8 collection', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. add (\\n9 documents =[\"Lâ€™IA vectorielle est puissante \", \"Le chat mange des\\ncroquettes \"],\\n10 metadatas =[{ \" source \": \" cours \"}, {\" source \": \" vie \"}] ,\\n11 ids =[\" id1 \", \" id2 \"]\\n12 )\\n13\\n14 # Recherche sÃ© mantique (Le plus proche voisin )\\n15 results = collection . query (\\n16 query_texts =[\" Base de donn Ã©es vecteur \"],\\n17 n_results =1\\n18 )\\n19\\nListing 4.1 â€“ Exemple de code ChromaDB pour la recherche sÃ©mantique\\nLâ€™intelligence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©-\\nment notre sociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique permettent aux ordina-\\nteurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque\\ntÃ¢che. Dans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vecto-\\nrielles jouent un rÃ´le crucial pour traiter des informations non structurÃ©es. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©-\\ncier la mise en page sans Ãªtre distrait par du latin', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Les avancÃ©es technologiques rÃ©centes,\\nnotamment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage\\nmassifs (LLM).\\nLâ€™intelligence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©-\\nment notre sociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique permettent aux ordina-\\nteurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque\\ntÃ¢che. Dans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vecto-\\nrielles jouent un rÃ´le crucial pour traiter des informations non structurÃ©es. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©-\\ncier la mise en page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques rÃ©centes,\\nnotamment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage\\nmassifs (LLM)', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='.\\n9\\n\\n[PAGE 11]\\n\\nÃ‰tat de lâ€™art Rapport IA 2024\\nLâ€™intelligence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©-\\nment notre sociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique permettent aux ordina-\\nteurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque\\ntÃ¢che. Dans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vecto-\\nrielles jouent un rÃ´le crucial pour traiter des informations non structurÃ©es. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©-\\ncier la mise en page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques rÃ©centes,\\nnotamment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage\\nmassifs (LLM).\\nLâ€™intelligence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©-\\nment notre sociÃ©tÃ©', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Les algorithmes dâ€™apprentissage automatique permettent aux ordina-\\nteurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque\\ntÃ¢che. Dans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vecto-\\nrielles jouent un rÃ´le crucial pour traiter des informations non structurÃ©es. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©-\\ncier la mise en page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques rÃ©centes,\\nnotamment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage\\nmassifs (LLM).\\n10\\n\\n[PAGE 12]\\n\\nChapitre 5\\nDÃ©ï¬s Ã‰thiques et Avenir\\n5.1 Biais Algorithmiques et Ã‰quitÃ©\\nLes modÃ¨les dâ€™IA peuvent reproduire, voire ampliï¬er, les biais prÃ©sents dans leurs\\ndonnÃ©es dâ€™entraÃ®nement. Lâ€™intelligence artiï¬cielle est un domaine en pleine expansion\\nqui transforme profondÃ©ment notre sociÃ©tÃ©', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Les algorithmes dâ€™apprentissage automatique\\npermettent aux ordinateurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement pro-\\ngrammÃ©s pour chaque tÃ¢che. Dans ce contexte, les rÃ©seaux de neurones profonds et les\\nbases de donnÃ©es vectorielles jouent un rÃ´le crucial pour traiter des informations non\\nstructurÃ©es. Ce paragraphe sert de remplissage pour simuler le volume de texte du rap-\\nport ï¬nal, permettant dâ€™apprÃ©cier la mise en page sans Ãªtre distrait par du latin. Les\\navancÃ©es technologiques rÃ©centes, notamment les architectures Transformers, ont ouvert\\nla voie aux modÃ¨les de langage massifs (LLM).\\n5.2 RÃ©gulation (AI Act)\\nLâ€™Union EuropÃ©enne et dâ€™autres instances travaillent sur des cadres lÃ©gaux. Lâ€™intelli-\\ngence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©ment notre\\nsociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique permettent aux ordinateurs dâ€™ap-\\nprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque tÃ¢che', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Dans\\nce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vectorielles jouent\\nun rÃ´le crucial pour traiter des informations non structurÃ©es. Ce paragraphe sert de rem-\\nplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©cier la mise\\nen page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques rÃ©centes, notamment\\nles architectures Transformers, ont ouvert la voie aux modÃ¨les de langage massifs (LLM).\\nLâ€™intelligence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©-\\nment notre sociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique permettent aux ordina-\\nteurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque\\ntÃ¢che. Dans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vecto-\\nrielles jouent un rÃ´le crucial pour traiter des informations non structurÃ©es', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©-\\ncier la mise en page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques rÃ©centes,\\nnotamment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage\\nmassifs (LLM).\\n11\\n\\n[PAGE 13]\\n\\nChapitre 6\\nConclusion\\nEn conclusion, lâ€™intelligence artiï¬cielle traverse une phase de maturation industrielle.\\nLâ€™intÃ©gration des bases de donnÃ©es vectorielles comme mÃ©moire Ã  long terme pour les\\nLLM ouvre la voie Ã  des agents autonomes plus performants et contextuels. Lâ€™intelli-\\ngence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©ment notre\\nsociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique permettent aux ordinateurs dâ€™ap-\\nprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque tÃ¢che', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Dans\\nce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vectorielles jouent\\nun rÃ´le crucial pour traiter des informations non structurÃ©es. Ce paragraphe sert de rem-\\nplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©cier la mise\\nen page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques rÃ©centes, notamment\\nles architectures Transformers, ont ouvert la voie aux modÃ¨les de langage massifs (LLM).\\n12\\n\\n[PAGE 14]\\n\\nBibliographie\\n[1] Vaswani, A., et al. (2017).Attention Is All You Need(Lâ€™attention est tout ce dont\\nvous avez besoin). Advances in Neural Information Processing Systems.\\n[2] LeCun, Y., Bengio, Y., & Hinton, G. (2015).Deep learning(Lâ€™apprentissage profond).\\nNature, 521(7553), 436-444.\\n[3] Johnson, J., Douze, M., & JÃ©gou, H. (2017).Billion-scale similarity search with GPUs\\n(Recherche de similaritÃ© Ã  lâ€™Ã©chelle du milliard avec GPU). IEEE Transactions on Big\\nData.\\n13\\n\\n[PAGE 15]', metadata={'source': 'embedding_course_by_koulou.pdf'})]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e3c8d4",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"faiss chroma.jpg\" alt=\"Logo Python\" width=\"1800\" height=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e92ee7",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; gap: 20px;\">\n",
    "\n",
    "  <!-- Colonne 1 -->\n",
    "  <div style=\"flex: 1; padding: 15px; border: 2px solid #e65100; border-radius: 10px; background-color: #fff0e0; box-shadow: 2px 2px 8px rgba(0,0,0,0.1); color:#212121;\">\n",
    "  \n",
    "  ## 1. Introduction GÃ©nÃ©rale : Les Bases de DonnÃ©es Vectorielles\n",
    "\n",
    "  **Qu'est-ce que c'est ?**  \n",
    "  Une base de donnÃ©es vectorielle est un systÃ¨me optimisÃ© pour stocker et interroger des *vecteurs* (embeddings).\n",
    "\n",
    "  **Pourquoi en avons-nous besoin ?**  \n",
    "  Les bases traditionnelles (SQL, NoSQL) gÃ¨rent bien les correspondances exactes, mais Ã©chouent sur la *similaritÃ©*.  \n",
    "\n",
    "  Avec lâ€™essor de lâ€™IA, les donnÃ©es complexes (texte, images, audio) sont transformÃ©es en vecteurs.  \n",
    "  âœ¨ Des concepts proches se retrouvent mathÃ©matiquement voisins dans un espace multidimensionnel.  \n",
    "\n",
    "  ğŸ‘‰ Fonction principale : **Vector Search**.  \n",
    "  Exemple : un vecteur \"chat\" renvoie les images les plus proches dâ€™un chat, mÃªme sans mot-clÃ©.  \n",
    "\n",
    "  Câ€™est le cÅ“ur des applications modernes comme le **RAG (Retrieval-Augmented Generation)**, qui permet aux LLM dâ€™accÃ©der Ã  vos documents.  \n",
    "\n",
    "  </div>\n",
    "\n",
    "  <!-- Colonne 2 -->\n",
    "  <div style=\"flex: 1; padding: 15px; border: 2px solid #e65100; border-radius: 10px; background-color: #fff0e0; box-shadow: 2px 2px 8px rgba(0,0,0,0.1); color:#212121;\">\n",
    "  \n",
    "  ## 2. FAISS et ChromaDB : Solutions orientÃ©es \"DÃ©veloppement Local\"\n",
    "\n",
    "  **FAISS (Facebook AI Similarity Search)**  \n",
    "  - Origine : Meta (Facebook AI Research).  \n",
    "  - Nature : BibliothÃ¨que C++/Python, ultra performante.  \n",
    "  - Forces :  \n",
    "    - ğŸš€ Recherche sur milliards de vecteurs.  \n",
    "    - âš¡ Optimisation GPU.  \n",
    "  - Faiblesses :  \n",
    "    - Bas niveau, pas de stockage persistant.  \n",
    "    - Risque de perte si non sauvegardÃ©.  \n",
    "\n",
    "  **ChromaDB**  \n",
    "  - Origine : Projet Open Source rÃ©cent.  \n",
    "  - Nature : Base vectorielle simple et rapide.  \n",
    "  - Forces :  \n",
    "    - ğŸ¯ Installation facile (`pip install chromadb`).  \n",
    "    - ğŸ”„ Vectorisation intÃ©grÃ©e.  \n",
    "  - Faiblesses :  \n",
    "    - IdÃ©al pour petits/moyens projets.  \n",
    "    - Moins robuste pour tÃ©raoctets en production.  \n",
    "\n",
    "  </div>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e1ff90",
   "metadata": {},
   "source": [
    "## FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc849efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def build_faiss_index(chunks):\n",
    "    \"\"\"\n",
    "    Construit un index FAISS Ã  partir de chunks dÃ©jÃ  prÃ©parÃ©s.\n",
    "    Utilise un modÃ¨le gratuit Hugging Face pour gÃ©nÃ©rer les embeddings.\n",
    "    \"\"\"\n",
    "    # 1. CrÃ©er les embeddings avec Hugging Face\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"  # modÃ¨le open-source ou openaiembeddings\n",
    "    )\n",
    "\n",
    "    # 2. Construire lâ€™index FAISS\n",
    "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "    return vectorstore\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a413f039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ultra Tech\\anaconda3\\envs\\llm_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RÃ©sultat 1 ---\n",
      ".\n",
      "8\n",
      "\n",
      "[PAGE 10]\n",
      "\n",
      "Ã‰tat de lâ€™art Rapport IA 2024\n",
      "4.3.1 Comparatif Technique\n",
      "â€” FAISS (Meta): BibliothÃ¨que bas niveau, optimisÃ©e pour la recherche par simila-\n",
      "ritÃ© dense et le clustering. Utilise lâ€™indexation IVF.\n",
      "â€” ChromaDB : Solution open-source conviviale pour les dÃ©veloppeurs, idÃ©ale pour\n",
      "les applications LLM lÃ©gÃ¨res et le prototypage.\n",
      "â€” Pinecone / Qdrant: Solutions gÃ©rÃ©es (SaaS) ou robustes (Rust) pour la produc-\n",
      "tion Ã  grande Ã©chelle.\n",
      "4.3.2 Exemple dâ€™implÃ©mentation (Python)\n",
      "Voici comment on ini ...\n",
      "--- RÃ©sultat 2 ---\n",
      ". Dans ce contexte, les rÃ©seaux de neurones profonds et les\n",
      "bases de donnÃ©es vectorielles jouent un rÃ´le crucial pour traiter des informations non\n",
      "structurÃ©es. Ce paragraphe sert de remplissage pour simuler le volume de texte du rap-\n",
      "port ï¬nal, permettant dâ€™apprÃ©cier la mise en page sans Ãªtre distrait par du latin. Les\n",
      "avancÃ©es technologiques rÃ©centes, notamment les architectures Transformers, ont ouvert\n",
      "la voie aux modÃ¨les de langage massifs (LLM).\n",
      "4.2 Mesure de la SimilaritÃ©\n",
      "Comme vu prÃ©cÃ©demm ...\n",
      "--- RÃ©sultat 3 ---\n",
      ". add (\n",
      "9 documents =[\"Lâ€™IA vectorielle est puissante \", \"Le chat mange des\n",
      "croquettes \"],\n",
      "10 metadatas =[{ \" source \": \" cours \"}, {\" source \": \" vie \"}] ,\n",
      "11 ids =[\" id1 \", \" id2 \"]\n",
      "12 )\n",
      "13\n",
      "14 # Recherche sÃ© mantique (Le plus proche voisin )\n",
      "15 results = collection . query (\n",
      "16 query_texts =[\" Base de donn Ã©es vecteur \"],\n",
      "17 n_results =1\n",
      "18 )\n",
      "19\n",
      "Listing 4.1 â€“ Exemple de code ChromaDB pour la recherche sÃ©mantique\n",
      "Lâ€™intelligence artiï¬cielle est un domaine en pleine expansion qui transforme profo ...\n",
      "--- RÃ©sultat 4 ---\n",
      ". Ce paragraphe\n",
      "sert de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©-\n",
      "cier la mise en page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques rÃ©centes,\n",
      "notamment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage\n",
      "massifs (LLM).\n",
      "7\n",
      "\n",
      "[PAGE 9]\n",
      "\n",
      "Chapitre 4\n",
      "Infrastructure de lâ€™IA : Embeddings et\n",
      "Bases Vectorielles\n",
      "Câ€™est ici que rÃ©side le cÅ“ur des systÃ¨mes RAG (GÃ©nÃ©ration AugmentÃ©e par la RÃ©cu-\n",
      "pÃ©ration) modernes.\n",
      "4.1 Le Concept dâ€™Embe ...\n",
      "--- RÃ©sultat 5 ---\n",
      ". Les algorithmes dâ€™apprentissage automatique permettent aux ordinateurs\n",
      "dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque tÃ¢che.\n",
      "Dans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vectorielles\n",
      "jouent un rÃ´le crucial pour traiter des informations non structurÃ©es. Ce paragraphe sert\n",
      "de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©cier la\n",
      "mise en page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques  ...\n",
      "--- RÃ©sultat 6 ---\n",
      ". Les avancÃ©es technologiques rÃ©centes,\n",
      "notamment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage\n",
      "massifs (LLM).\n",
      "Lâ€™intelligence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©-\n",
      "ment notre sociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique permettent aux ordina-\n",
      "teurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque\n",
      "tÃ¢che. Dans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vecto-\n",
      "rielles jouen ...\n",
      "--- RÃ©sultat 7 ---\n",
      ". Les avancÃ©es technologiques rÃ©centes,\n",
      "notamment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage\n",
      "massifs (LLM).\n",
      "Lâ€™intelligence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©-\n",
      "ment notre sociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique permettent aux ordina-\n",
      "teurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque\n",
      "tÃ¢che. Dans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vecto-\n",
      "rielles jouen ...\n",
      "--- RÃ©sultat 8 ---\n",
      ". Lâ€™intelligence artiï¬cielle est un domaine en pleine expansion\n",
      "qui transforme profondÃ©ment notre sociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique\n",
      "permettent aux ordinateurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement pro-\n",
      "grammÃ©s pour chaque tÃ¢che. Dans ce contexte, les rÃ©seaux de neurones profonds et les\n",
      "bases de donnÃ©es vectorielles jouent un rÃ´le crucial pour traiter des informations non\n",
      "structurÃ©es. Ce paragraphe sert de remplissage pour simuler le volume de texte du rap-\n",
      "po ...\n",
      "--- RÃ©sultat 9 ---\n",
      ". Les premiers systÃ¨mes experts ont laissÃ© place aux rÃ©seaux de neurones. Lâ€™intelli-\n",
      "gence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©ment notre\n",
      "sociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique permettent aux ordinateurs dâ€™ap-\n",
      "prendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque tÃ¢che. Dans\n",
      "ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vectorielles jouent\n",
      "un rÃ´le crucial pour traiter des informations non structurÃ©es. Ce ...\n",
      "--- RÃ©sultat 10 ---\n",
      ". Les algorithmes dâ€™apprentissage automatique permettent aux ordina-\n",
      "teurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque\n",
      "tÃ¢che. Dans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vecto-\n",
      "rielles jouent un rÃ´le crucial pour traiter des informations non structurÃ©es. Ce paragraphe\n",
      "sert de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©-\n",
      "cier la mise en page sans Ãªtre distrait par du latin. Les avancÃ©es technolog ...\n"
     ]
    }
   ],
   "source": [
    "# Supposons que tu as dÃ©jÃ  tes chunks :\n",
    "# chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "faiss_index = build_faiss_index(pdf_chunks)\n",
    "\n",
    "# Faire une requÃªte\n",
    "query = \"Explique-moi la diffÃ©rence entre FAISS et ChromaDB\"\n",
    "results = faiss_index.similarity_search(query, k=10)\n",
    "\n",
    "# Afficher les rÃ©sultats\n",
    "for i, res in enumerate(results):\n",
    "    print(f\"--- RÃ©sultat {i+1} ---\")\n",
    "    print(res.page_content[:500], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f004caa",
   "metadata": {},
   "source": [
    "<img src=\"reranking.jpg\" alt=\"Logo Python\" width=\"1800\" height=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3df822",
   "metadata": {},
   "source": [
    "## Re-Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc90c0f0",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; gap: 20px;\">\n",
    "\n",
    "  <!-- Colonne 1 -->\n",
    "  <div style=\"flex: 1; padding: 15px; border: 2px solid #e65100; border-radius: 10px; background-color: #fff0e0; box-shadow: 2px 2px 8px rgba(0,0,0,0.1); color:#212121;\">\n",
    "  \n",
    "  ## 1. Reranking : Concept et Pourquoi lâ€™utiliser\n",
    "\n",
    "  **Qu'estâ€‘ce que le reranking ?**  \n",
    "  Le *reranking* est une Ã©tape secondaire qui rÃ©ordonne une liste de rÃ©sultats initialement rÃ©cupÃ©rÃ©s (par ex. via FAISS) en utilisant un modÃ¨le plus fin ou une mÃ©thode diffÃ©rente pour mieux estimer la pertinence.\n",
    "\n",
    "  **Pourquoi lâ€™ajouter ?**  \n",
    "  - Les recherches vectorielles (FAISS, ChromaDB) fournissent un bon *prÃ©croisement* basÃ© sur la proximitÃ© dans lâ€™espace des embeddings.  \n",
    "  - Le reranking applique une Ã©valuation sÃ©mantique plus riche (modÃ¨les de rerank, cross-encoders) pour corriger les faux positifs et amÃ©liorer la prÃ©cision en tÃªte de liste.\n",
    "\n",
    "  **Flux typique**  \n",
    "  1. **Retrieval** : rÃ©cupÃ©rer topâ€‘k via FAISS (rapide).  \n",
    "  2. **Rerank** : envoyer ces k documents + la requÃªte Ã  un modÃ¨le de reranking.  \n",
    "  3. **RÃ©ordonnancement** : trier par score de rerank et retourner topâ€‘n final.\n",
    "\n",
    "  **Avantages**  \n",
    "  - ğŸ” Meilleure prÃ©cision en tÃªte de rÃ©sultats.  \n",
    "  - âš–ï¸ Moins de bruit : on rÃ©duit les documents proches mais non pertinents.  \n",
    "  - ğŸ’¡ ComplÃ©mentaritÃ© : combine vitesse (FAISS) et finesse (reranker).\n",
    "\n",
    "  **Limites**  \n",
    "  - â±ï¸ CoÃ»t et latence supplÃ©mentaires (appel au modÃ¨le de rerank).  \n",
    "  - ğŸ’¸ CoÃ»t monÃ©taire si le reranker est un service payant.  \n",
    "  - ğŸ” NÃ©cessite un bon prÃ©traitement (nettoyage, dÃ©coupage) pour Ãªtre efficace.\n",
    "\n",
    "  **Quand lâ€™utiliser ?**  \n",
    "  - Quand la qualitÃ© des 3â€“5 premiers rÃ©sultats est critique (chatbot, RAG, FAQ).  \n",
    "  - Quand la base contient beaucoup de documents proches sÃ©mantiquement mais peu pertinents.\n",
    "\n",
    "  </div>\n",
    "\n",
    "  <!-- Colonne 2 -->\n",
    "  <div style=\"flex: 1; padding: 15px; border: 2px solid #e65100; border-radius: 10px; background-color: #fff0e0; box-shadow: 2px 2px 8px rgba(0,0,0,0.1); color:#212121;\">\n",
    "  \n",
    "  ## 2. Cohere et Reranking : Mise en Å“uvre pratique\n",
    "\n",
    "  **Pourquoi Cohere pour le rerank ?**  \n",
    "  Cohere propose des modÃ¨les dÃ©diÃ©s au *reranking* (crossâ€‘encoder style) qui comparent directement la requÃªte et chaque document pour produire un score de pertinence fin.\n",
    "\n",
    "  **Pattern dâ€™intÃ©gration**  \n",
    "  - **Ã‰tape 1** : FAISS â†’ `similarity_search(query, k=K)`  \n",
    "  - **Ã‰tape 2** : Extraire `page_content` des K rÃ©sultats.  \n",
    "  - **Ã‰tape 3** : Appel Cohere Rerank avec `query` + `documents`.  \n",
    "  - **Ã‰tape 4** : RÃ©ordonner les objets FAISS selon `relevance_score` renvoyÃ© par Cohere.\n",
    "\n",
    "  **Bonnes pratiques**  \n",
    "  - **Choisir le modÃ¨le** : `rerank-multilingual` si documents en franÃ§ais; `rerank-english` pour lâ€™anglais.  \n",
    "  - **Limiter K** : 10â€“50 selon latence/coÃ»t.  \n",
    "  - **Fallback** : prÃ©voir un retour aux rÃ©sultats FAISS si lâ€™API Ã©choue.  \n",
    "  - **Conserver mÃ©tadonnÃ©es** : renvoyer lâ€™objet FAISS original pour garder source, offset, page, etc.  \n",
    "  - **Batching** : grouper les appels si possible pour rÃ©duire latence et coÃ»t.\n",
    "\n",
    "  **Exemples dâ€™amÃ©liorations observables**  \n",
    "  - Augmentation de la prÃ©cision @1 et @3.  \n",
    "  - RÃ©duction des hallucinations dans les rÃ©ponses RAG.  \n",
    "  - Meilleure cohÃ©rence des extraits utilisÃ©s pour la gÃ©nÃ©ration.\n",
    "\n",
    "  **Risques et mitigations**  \n",
    "  - **Latence** â†’ mettre en cache les reranks frÃ©quents.  \n",
    "  - **CoÃ»t** â†’ nâ€™utiliser le rerank que pour les requÃªtes critiques.  \n",
    "  - **Biais** â†’ tester le reranker sur jeux de requÃªtes reprÃ©sentatifs.\n",
    "\n",
    "  **Astuce rapide**  \n",
    "  Combine un score FAISS (similitude) et le score Cohere (rerank) via une pondÃ©ration pour tirer parti des deux signaux avant le tri final.\n",
    "\n",
    "  </div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ad05b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Configuration globale ===\n",
    "# Remplace par ta clÃ© API Cohere\n",
    "COHERE_API_KEY = \"votre cle cohere\"  # âš ï¸ Ã€ remplacer ou Ã  charger depuis .env / config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2e380e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "\n",
    "# Initialiser le client Cohere\n",
    "co = cohere.Client(COHERE_API_KEY)\n",
    "\n",
    "def rerank_with_cohere(\n",
    "    query: str,\n",
    "    faiss_results: List[Any],\n",
    "    top_n: int = 5,\n",
    "    model: str = \"rerank-multilingual-v2.0\"\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Rerank les rÃ©sultats FAISS avec l'API Cohere Rerank.\n",
    "    \"\"\"\n",
    "    if not faiss_results:\n",
    "        return []\n",
    "\n",
    "    documents = []\n",
    "    for i, res in enumerate(faiss_results):\n",
    "        text = getattr(res, \"page_content\", None) or getattr(res, \"content\", None) or str(res)\n",
    "        documents.append(text)\n",
    "\n",
    "    try:\n",
    "        response = co.rerank(\n",
    "            model=model,\n",
    "            query=query,\n",
    "            documents=documents,\n",
    "            top_n=min(top_n, len(documents))\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"[âš ï¸ Warning] Cohere rerank failed: {e}\")\n",
    "        return [\n",
    "            {\"doc\": faiss_results[i], \"original_index\": i, \"score\": None}\n",
    "            for i in range(min(top_n, len(faiss_results)))\n",
    "        ]\n",
    "\n",
    "    results_list = getattr(response, \"results\", None) or response\n",
    "\n",
    "    reranked = []\n",
    "    for item in results_list:\n",
    "        idx = getattr(item, \"index\", None)\n",
    "        score = getattr(item, \"relevance_score\", None) or getattr(item, \"score\", None)\n",
    "\n",
    "        if idx is None or idx >= len(faiss_results):\n",
    "            continue\n",
    "\n",
    "        reranked.append({\n",
    "            \"doc\": faiss_results[idx],\n",
    "            \"original_index\": idx,\n",
    "            \"score\": float(score) if score is not None else None\n",
    "        })\n",
    "\n",
    "    return reranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c915a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[âš ï¸ Warning] Cohere rerank failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-encoding': 'gzip', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 GMT', 'pragma': 'no-cache', 'vary': 'Origin,Accept-Encoding', 'x-accel-expires': '0', 'x-debug-trace-id': 'f95aced5c4f3235587eb9d12b770dfc8', 'x-endpoint-monthly-call-limit': '1000', 'x-trial-endpoint-call-limit': '10', 'x-trial-endpoint-call-remaining': '9', 'date': 'Sat, 10 Jan 2026 13:01:20 GMT', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000', 'transfer-encoding': 'chunked'}, status_code: 404, body: {'id': 'c0eb073e-b61e-4b46-84b1-8916c74b6e0c', 'message': \"model 'rerank-multilingual-v2.0' was sunset on December 1st, 2025. See https://docs.cohere.com/docs/models#rerank for a list of models you can use instead.\"}\n",
      "5 rÃ©sultats aprÃ¨s reranking avec Cohere :\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Exemple d'utilisation ===\n",
    "# âš ï¸ Assure-toi que `faiss_index` est dÃ©fini avant cet appel\n",
    "# Exemple fictif :\n",
    "# from some_module import faiss_index\n",
    "\n",
    "query = \"Explique-moi la diffÃ©rence entre FAISS et ChromaDB\"\n",
    "faiss_results = faiss_index.similarity_search(query, k=10)\n",
    "\n",
    "reranked_results = rerank_with_cohere(query, faiss_results, top_n=5)\n",
    "\n",
    "print(len(reranked_results), \"rÃ©sultats aprÃ¨s reranking avec Cohere :\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75341be2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'doc': Document(page_content='.\\n8\\n\\n[PAGE 10]\\n\\nÃ‰tat de lâ€™art Rapport IA 2024\\n4.3.1 Comparatif Technique\\nâ€” FAISS (Meta): BibliothÃ¨que bas niveau, optimisÃ©e pour la recherche par simila-\\nritÃ© dense et le clustering. Utilise lâ€™indexation IVF.\\nâ€” ChromaDB : Solution open-source conviviale pour les dÃ©veloppeurs, idÃ©ale pour\\nles applications LLM lÃ©gÃ¨res et le prototypage.\\nâ€” Pinecone / Qdrant: Solutions gÃ©rÃ©es (SaaS) ou robustes (Rust) pour la produc-\\ntion Ã  grande Ã©chelle.\\n4.3.2 Exemple dâ€™implÃ©mentation (Python)\\nVoici comment on initialise une recherche simple avec la librairie ChromaDB :\\n1 import chromadb\\n2\\n3 # Initialisation du client (en mÃ© moire pour lâ€™ exemple )\\n4 client = chromadb . Client ()\\n5 collection = client . create_collection (\" mon_dataset_ia \")\\n6\\n7 # Ajout de documents ( texte brut + mÃ© tadonn Ã©es)\\n8 collection', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       "  'original_index': 0,\n",
       "  'score': None},\n",
       " {'doc': Document(page_content='. Dans ce contexte, les rÃ©seaux de neurones profonds et les\\nbases de donnÃ©es vectorielles jouent un rÃ´le crucial pour traiter des informations non\\nstructurÃ©es. Ce paragraphe sert de remplissage pour simuler le volume de texte du rap-\\nport ï¬nal, permettant dâ€™apprÃ©cier la mise en page sans Ãªtre distrait par du latin. Les\\navancÃ©es technologiques rÃ©centes, notamment les architectures Transformers, ont ouvert\\nla voie aux modÃ¨les de langage massifs (LLM).\\n4.2 Mesure de la SimilaritÃ©\\nComme vu prÃ©cÃ©demment, la similaritÃ© cosinus est la mÃ©trique standard pour Ã©valuer\\nla proximitÃ© sÃ©mantique :\\nsim(A,B) = cos(Î¸) = AÂ·B\\nâˆ¥Aâˆ¥âˆ¥Bâˆ¥=\\nâˆ‘n\\ni=1 AiBiâˆšâˆ‘n\\ni=1 A2\\ni\\nâˆšâˆ‘n\\ni=1 B2\\ni\\n(4.1)\\n4.3 Les Bases de DonnÃ©es Vectorielles\\nPour gÃ©rer des millions de vecteurs, des solutions spÃ©cialisÃ©es sont nÃ©cessaires. Les\\nbases de donnÃ©es relationnelles classiques (SQL) ne sont pas optimisÃ©es pour ce type de\\ncalcul.\\n8\\n\\n[PAGE 10]\\n\\nÃ‰tat de lâ€™art Rapport IA 2024\\n4.3', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       "  'original_index': 1,\n",
       "  'score': None},\n",
       " {'doc': Document(page_content='. add (\\n9 documents =[\"Lâ€™IA vectorielle est puissante \", \"Le chat mange des\\ncroquettes \"],\\n10 metadatas =[{ \" source \": \" cours \"}, {\" source \": \" vie \"}] ,\\n11 ids =[\" id1 \", \" id2 \"]\\n12 )\\n13\\n14 # Recherche sÃ© mantique (Le plus proche voisin )\\n15 results = collection . query (\\n16 query_texts =[\" Base de donn Ã©es vecteur \"],\\n17 n_results =1\\n18 )\\n19\\nListing 4.1 â€“ Exemple de code ChromaDB pour la recherche sÃ©mantique\\nLâ€™intelligence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©-\\nment notre sociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique permettent aux ordina-\\nteurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque\\ntÃ¢che. Dans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vecto-\\nrielles jouent un rÃ´le crucial pour traiter des informations non structurÃ©es. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©-\\ncier la mise en page sans Ãªtre distrait par du latin', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       "  'original_index': 2,\n",
       "  'score': None},\n",
       " {'doc': Document(page_content='. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©-\\ncier la mise en page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques rÃ©centes,\\nnotamment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage\\nmassifs (LLM).\\n7\\n\\n[PAGE 9]\\n\\nChapitre 4\\nInfrastructure de lâ€™IA : Embeddings et\\nBases Vectorielles\\nCâ€™est ici que rÃ©side le cÅ“ur des systÃ¨mes RAG (GÃ©nÃ©ration AugmentÃ©e par la RÃ©cu-\\npÃ©ration) modernes.\\n4.1 Le Concept dâ€™Embedding (Plongement Lexical)\\nLes embeddings transforment des donnÃ©es non structurÃ©es (texte, image, son) en vec-\\nteurs denses de dimensiond. Lâ€™intelligence artiï¬cielle est un domaine en pleine expansion\\nqui transforme profondÃ©ment notre sociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique\\npermettent aux ordinateurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement pro-\\ngrammÃ©s pour chaque tÃ¢che', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       "  'original_index': 3,\n",
       "  'score': None},\n",
       " {'doc': Document(page_content='. Les algorithmes dâ€™apprentissage automatique permettent aux ordinateurs\\ndâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque tÃ¢che.\\nDans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vectorielles\\njouent un rÃ´le crucial pour traiter des informations non structurÃ©es. Ce paragraphe sert\\nde remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©cier la\\nmise en page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques rÃ©centes, notam-\\nment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage massifs\\n(LLM).\\n3.2 Les Transformers et le MÃ©canisme dâ€™Attention\\nLâ€™innovation majeure de lâ€™article fondateur de 2017 a rÃ©volutionnÃ© le traitement du\\nlangage naturel (NLP). Lâ€™attention se calcule ainsi :\\nAttention(Q,K,V ) =softmax\\n(QKT\\nâˆšdk\\n)\\nV (3.1)\\nCette architecture permet de parallÃ©liser les calculs, contrairement aux rÃ©seaux rÃ©cur-\\nrents (RNN) prÃ©cÃ©dents', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       "  'original_index': 4,\n",
       "  'score': None}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reranked_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e2cfb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RÃ©sultats RerankÃ©s ===\n",
      "\n",
      "--- RÃ©sultat 1 --- (Score: N/A)\n",
      ".\n",
      "8\n",
      "\n",
      "[PAGE 10]\n",
      "\n",
      "Ã‰tat de lâ€™art Rapport IA 2024\n",
      "4.3.1 Comparatif Technique\n",
      "â€” FAISS (Meta): BibliothÃ¨que bas niveau, optimisÃ©e pour la recherche par simila-\n",
      "ritÃ© dense et le clustering. Utilise lâ€™indexation IVF.\n",
      "â€” ChromaDB : Solution open-source conviviale pour les dÃ©veloppeurs, idÃ©ale pour\n",
      "les applications LLM lÃ©gÃ¨res et le prototypage.\n",
      "â€” Pinecone / Qdrant: Solutions gÃ©rÃ©es (SaaS) ou robustes (Rust) pour la produc-\n",
      "tion Ã  grande Ã©chelle.\n",
      "4.3.2 Exemple dâ€™implÃ©mentation (Python)\n",
      "Voici comment on ini ...\n",
      "--- RÃ©sultat 2 --- (Score: N/A)\n",
      ". Dans ce contexte, les rÃ©seaux de neurones profonds et les\n",
      "bases de donnÃ©es vectorielles jouent un rÃ´le crucial pour traiter des informations non\n",
      "structurÃ©es. Ce paragraphe sert de remplissage pour simuler le volume de texte du rap-\n",
      "port ï¬nal, permettant dâ€™apprÃ©cier la mise en page sans Ãªtre distrait par du latin. Les\n",
      "avancÃ©es technologiques rÃ©centes, notamment les architectures Transformers, ont ouvert\n",
      "la voie aux modÃ¨les de langage massifs (LLM).\n",
      "4.2 Mesure de la SimilaritÃ©\n",
      "Comme vu prÃ©cÃ©demm ...\n",
      "--- RÃ©sultat 3 --- (Score: N/A)\n",
      ". add (\n",
      "9 documents =[\"Lâ€™IA vectorielle est puissante \", \"Le chat mange des\n",
      "croquettes \"],\n",
      "10 metadatas =[{ \" source \": \" cours \"}, {\" source \": \" vie \"}] ,\n",
      "11 ids =[\" id1 \", \" id2 \"]\n",
      "12 )\n",
      "13\n",
      "14 # Recherche sÃ© mantique (Le plus proche voisin )\n",
      "15 results = collection . query (\n",
      "16 query_texts =[\" Base de donn Ã©es vecteur \"],\n",
      "17 n_results =1\n",
      "18 )\n",
      "19\n",
      "Listing 4.1 â€“ Exemple de code ChromaDB pour la recherche sÃ©mantique\n",
      "Lâ€™intelligence artiï¬cielle est un domaine en pleine expansion qui transforme profo ...\n",
      "--- RÃ©sultat 4 --- (Score: N/A)\n",
      ". Ce paragraphe\n",
      "sert de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©-\n",
      "cier la mise en page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques rÃ©centes,\n",
      "notamment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage\n",
      "massifs (LLM).\n",
      "7\n",
      "\n",
      "[PAGE 9]\n",
      "\n",
      "Chapitre 4\n",
      "Infrastructure de lâ€™IA : Embeddings et\n",
      "Bases Vectorielles\n",
      "Câ€™est ici que rÃ©side le cÅ“ur des systÃ¨mes RAG (GÃ©nÃ©ration AugmentÃ©e par la RÃ©cu-\n",
      "pÃ©ration) modernes.\n",
      "4.1 Le Concept dâ€™Embe ...\n",
      "--- RÃ©sultat 5 --- (Score: N/A)\n",
      ". Les algorithmes dâ€™apprentissage automatique permettent aux ordinateurs\n",
      "dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque tÃ¢che.\n",
      "Dans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vectorielles\n",
      "jouent un rÃ´le crucial pour traiter des informations non structurÃ©es. Ce paragraphe sert\n",
      "de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©cier la\n",
      "mise en page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques  ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"\\n=== RÃ©sultats RerankÃ©s ===\\n\")\n",
    "for i, res in enumerate(reranked_results, 1):\n",
    "    score_str = f\"{res['score']:.4f}\" if res['score'] is not None else \"N/A\"\n",
    "    print(f\"--- RÃ©sultat {i} --- (Score: {score_str})\")\n",
    "    content = getattr(res[\"doc\"], \"page_content\", \"\") or getattr(res[\"doc\"], \"content\", \"\")\n",
    "    print(content[:500], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf7afb9",
   "metadata": {},
   "source": [
    "## petit prompt engeneering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913614ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# === Configuration OpenAI ===\n",
    "OPENAI_API_KEY = \"votre cle api_openai\"  \n",
    "# âš ï¸ Ã€ remplacer ou charger depuis .env\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# Optionnel : si tu veux utiliser un modÃ¨le diffÃ©rent\n",
    "GPT_MODEL = \"gpt-4o-mini\"  # ou \"gpt-3.5-turbo\", \"gpt-4-turbo\", etc.\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)  # âœ… Client instanc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb7626f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[âš ï¸ Warning] Cohere rerank failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-encoding': 'gzip', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 GMT', 'pragma': 'no-cache', 'vary': 'Origin,Accept-Encoding', 'x-accel-expires': '0', 'x-debug-trace-id': '3e6e9c8c29960a58b3444ffd2f552571', 'x-endpoint-monthly-call-limit': '1000', 'x-trial-endpoint-call-limit': '10', 'x-trial-endpoint-call-remaining': '9', 'date': 'Sat, 10 Jan 2026 13:20:52 GMT', 'x-envoy-upstream-service-time': '8', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000', 'transfer-encoding': 'chunked'}, status_code: 404, body: {'id': '30a7f5d7-df98-49c5-869e-75266c691983', 'message': \"model 'rerank-multilingual-v2.0' was sunset on December 1st, 2025. See https://docs.cohere.com/docs/models#rerank for a list of models you can use instead.\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'doc': Document(page_content='.\\n8\\n\\n[PAGE 10]\\n\\nÃ‰tat de lâ€™art Rapport IA 2024\\n4.3.1 Comparatif Technique\\nâ€” FAISS (Meta): BibliothÃ¨que bas niveau, optimisÃ©e pour la recherche par simila-\\nritÃ© dense et le clustering. Utilise lâ€™indexation IVF.\\nâ€” ChromaDB : Solution open-source conviviale pour les dÃ©veloppeurs, idÃ©ale pour\\nles applications LLM lÃ©gÃ¨res et le prototypage.\\nâ€” Pinecone / Qdrant: Solutions gÃ©rÃ©es (SaaS) ou robustes (Rust) pour la produc-\\ntion Ã  grande Ã©chelle.\\n4.3.2 Exemple dâ€™implÃ©mentation (Python)\\nVoici comment on initialise une recherche simple avec la librairie ChromaDB :\\n1 import chromadb\\n2\\n3 # Initialisation du client (en mÃ© moire pour lâ€™ exemple )\\n4 client = chromadb . Client ()\\n5 collection = client . create_collection (\" mon_dataset_ia \")\\n6\\n7 # Ajout de documents ( texte brut + mÃ© tadonn Ã©es)\\n8 collection', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       "  'original_index': 0,\n",
       "  'score': None},\n",
       " {'doc': Document(page_content='. Dans ce contexte, les rÃ©seaux de neurones profonds et les\\nbases de donnÃ©es vectorielles jouent un rÃ´le crucial pour traiter des informations non\\nstructurÃ©es. Ce paragraphe sert de remplissage pour simuler le volume de texte du rap-\\nport ï¬nal, permettant dâ€™apprÃ©cier la mise en page sans Ãªtre distrait par du latin. Les\\navancÃ©es technologiques rÃ©centes, notamment les architectures Transformers, ont ouvert\\nla voie aux modÃ¨les de langage massifs (LLM).\\n4.2 Mesure de la SimilaritÃ©\\nComme vu prÃ©cÃ©demment, la similaritÃ© cosinus est la mÃ©trique standard pour Ã©valuer\\nla proximitÃ© sÃ©mantique :\\nsim(A,B) = cos(Î¸) = AÂ·B\\nâˆ¥Aâˆ¥âˆ¥Bâˆ¥=\\nâˆ‘n\\ni=1 AiBiâˆšâˆ‘n\\ni=1 A2\\ni\\nâˆšâˆ‘n\\ni=1 B2\\ni\\n(4.1)\\n4.3 Les Bases de DonnÃ©es Vectorielles\\nPour gÃ©rer des millions de vecteurs, des solutions spÃ©cialisÃ©es sont nÃ©cessaires. Les\\nbases de donnÃ©es relationnelles classiques (SQL) ne sont pas optimisÃ©es pour ce type de\\ncalcul.\\n8\\n\\n[PAGE 10]\\n\\nÃ‰tat de lâ€™art Rapport IA 2024\\n4.3', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       "  'original_index': 1,\n",
       "  'score': None},\n",
       " {'doc': Document(page_content='. add (\\n9 documents =[\"Lâ€™IA vectorielle est puissante \", \"Le chat mange des\\ncroquettes \"],\\n10 metadatas =[{ \" source \": \" cours \"}, {\" source \": \" vie \"}] ,\\n11 ids =[\" id1 \", \" id2 \"]\\n12 )\\n13\\n14 # Recherche sÃ© mantique (Le plus proche voisin )\\n15 results = collection . query (\\n16 query_texts =[\" Base de donn Ã©es vecteur \"],\\n17 n_results =1\\n18 )\\n19\\nListing 4.1 â€“ Exemple de code ChromaDB pour la recherche sÃ©mantique\\nLâ€™intelligence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©-\\nment notre sociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique permettent aux ordina-\\nteurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque\\ntÃ¢che. Dans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vecto-\\nrielles jouent un rÃ´le crucial pour traiter des informations non structurÃ©es. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©-\\ncier la mise en page sans Ãªtre distrait par du latin', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       "  'original_index': 2,\n",
       "  'score': None},\n",
       " {'doc': Document(page_content='. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©-\\ncier la mise en page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques rÃ©centes,\\nnotamment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage\\nmassifs (LLM).\\n7\\n\\n[PAGE 9]\\n\\nChapitre 4\\nInfrastructure de lâ€™IA : Embeddings et\\nBases Vectorielles\\nCâ€™est ici que rÃ©side le cÅ“ur des systÃ¨mes RAG (GÃ©nÃ©ration AugmentÃ©e par la RÃ©cu-\\npÃ©ration) modernes.\\n4.1 Le Concept dâ€™Embedding (Plongement Lexical)\\nLes embeddings transforment des donnÃ©es non structurÃ©es (texte, image, son) en vec-\\nteurs denses de dimensiond. Lâ€™intelligence artiï¬cielle est un domaine en pleine expansion\\nqui transforme profondÃ©ment notre sociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique\\npermettent aux ordinateurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement pro-\\ngrammÃ©s pour chaque tÃ¢che', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       "  'original_index': 3,\n",
       "  'score': None},\n",
       " {'doc': Document(page_content='. Les algorithmes dâ€™apprentissage automatique permettent aux ordinateurs\\ndâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque tÃ¢che.\\nDans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vectorielles\\njouent un rÃ´le crucial pour traiter des informations non structurÃ©es. Ce paragraphe sert\\nde remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©cier la\\nmise en page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques rÃ©centes, notam-\\nment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage massifs\\n(LLM).\\n3.2 Les Transformers et le MÃ©canisme dâ€™Attention\\nLâ€™innovation majeure de lâ€™article fondateur de 2017 a rÃ©volutionnÃ© le traitement du\\nlangage naturel (NLP). Lâ€™attention se calcule ainsi :\\nAttention(Q,K,V ) =softmax\\n(QKT\\nâˆšdk\\n)\\nV (3.1)\\nCette architecture permet de parallÃ©liser les calculs, contrairement aux rÃ©seaux rÃ©cur-\\nrents (RNN) prÃ©cÃ©dents', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       "  'original_index': 4,\n",
       "  'score': None}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def generate_answer_with_gpt(\n",
    "    query: str,\n",
    "    reranked_results: List[Dict[str, Any]],\n",
    "    system_prompt: Optional[str] = None,\n",
    "    max_tokens: int = 500,\n",
    "    temperature: float = 0.3\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    GÃ©nÃ¨re une rÃ©ponse synthÃ©tique Ã  partir des rÃ©sultats rerankÃ©s, en utilisant GPT (OpenAI v1+).\n",
    "    \"\"\"\n",
    "    if not reranked_results:\n",
    "        return \"Aucun rÃ©sultat disponible pour rÃ©pondre Ã  votre question.\"\n",
    "\n",
    "    context_parts = []\n",
    "    for i, item in enumerate(reranked_results, 1):\n",
    "        doc = item[\"doc\"]\n",
    "        content = getattr(doc, \"page_content\", \"\") or getattr(doc, \"content\", \"\")\n",
    "        score = item.get(\"score\")\n",
    "        original_idx = item.get(\"original_index\", \"??\")\n",
    "\n",
    "        score_str = f\"{score:.4f}\" if score is not None else \"N/A\"\n",
    "        context_parts.append(\n",
    "            f\"[Document {i} (Orig. idx: {original_idx}) - Score: {score_str}]\\n{content.strip()}\"\n",
    "        )\n",
    "\n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "    if system_prompt is None:\n",
    "        system_prompt = (\n",
    "            \"Tu es un assistant intelligent qui rÃ©pond aux questions en t'appuyant \"\n",
    "            \"sur des extraits de documents fournis. Sois clair, concis, et prÃ©cis. \"\n",
    "            \"Si tu ne trouves pas d'information pertinente, dis-le honnÃªtement.\"\n",
    "        )\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "Question : {query}\n",
    "\n",
    "Contexte (documents pertinents) :\n",
    "{context}\n",
    "\n",
    "RÃ©ponds de maniÃ¨re complÃ¨te, structurÃ©e, et adaptÃ©e Ã  la question.\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        completion = client.chat.completions.create(  # âœ… Nouvelle syntaxe OpenAI v1+\n",
    "            model=GPT_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            n=1\n",
    "        )\n",
    "\n",
    "        answer = completion.choices[0].message.content.strip()\n",
    "        return answer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[âš ï¸ Error] Ã‰chec de gÃ©nÃ©ration avec GPT : {e}\")\n",
    "        return \"DÃ©solÃ©, je n'ai pas pu gÃ©nÃ©rer de rÃ©ponse pour le moment.\"\n",
    "    \n",
    "    \n",
    "# Supposons que tu as dÃ©jÃ  exÃ©cutÃ© le reranking\n",
    "query = \"Explique-moi la diffÃ©rence entre FAISS et ChromaDB\"\n",
    "faiss_results = faiss_index.similarity_search(query, k=10)\n",
    "reranked = rerank_with_cohere(query, faiss_results, top_n=5)\n",
    "reranked    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e219b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RÃ©ponse finale gÃ©nÃ©rÃ©e par GPT ===\n",
      "\n",
      "FAISS (Facebook AI Similarity Search) et ChromaDB sont deux outils utilisÃ©s pour la recherche et la gestion de donnÃ©es vectorielles, mais ils diffÃ¨rent dans leur conception et leur utilisation.\n",
      "\n",
      "### FAISS\n",
      "- **DÃ©veloppeur** : Meta (anciennement Facebook).\n",
      "- **Type** : BibliothÃ¨que de bas niveau.\n",
      "- **FonctionnalitÃ©** : OptimisÃ©e pour la recherche par similaritÃ© dense et le clustering.\n",
      "- **Indexation** : Utilise des techniques avancÃ©es comme l'indexation IVF (Inverted File).\n",
      "- **Utilisation** : Plus adaptÃ©e pour des applications nÃ©cessitant une performance Ã©levÃ©e dans la recherche de similaritÃ© sur de grands ensembles de donnÃ©es.\n",
      "\n",
      "### ChromaDB\n",
      "- **Type** : Solution open-source.\n",
      "- **ConvivialitÃ©** : ConÃ§ue pour Ãªtre facile Ã  utiliser, particuliÃ¨rement pour les dÃ©veloppeurs.\n",
      "- **Applications** : IdÃ©ale pour des applications lÃ©gÃ¨res de modÃ¨les de langage (LLM) et pour le prototypage.\n",
      "- **FonctionnalitÃ©** : Permet l'ajout de documents et la recherche sÃ©mantique de maniÃ¨re intuitive, comme illustrÃ© dans les exemples de code fournis.\n",
      "\n",
      "### RÃ©sumÃ© des diffÃ©rences\n",
      "- **ComplexitÃ© et Performance** : FAISS est plus complexe et performant pour des tÃ¢ches de recherche avancÃ©es, tandis que ChromaDB est plus accessible et adaptÃ© pour des prototypes ou des applications moins exigeantes.\n",
      "- **Cible d'utilisateur** : FAISS s'adresse principalement aux utilisateurs ayant des besoins techniques spÃ©cifiques, alors que ChromaDB vise une audience plus large, y compris les dÃ©veloppeurs cherchant Ã  intÃ©grer rapidement des fonctionnalitÃ©s de recherche.\n",
      "\n",
      "En rÃ©sumÃ©, FAISS est une bibliothÃ¨que technique pour des usages avancÃ©s, tandis que ChromaDB est une solution plus conviviale pour le dÃ©veloppement rapide d'applications.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# GÃ©nÃ©rer la rÃ©ponse finale avec GPT\n",
    "final_answer = generate_answer_with_gpt(query, reranked)\n",
    "\n",
    "print(\"\\n=== RÃ©ponse finale gÃ©nÃ©rÃ©e par GPT ===\\n\")\n",
    "print(final_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104f9101",
   "metadata": {},
   "source": [
    "## FULL process avec Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2c93ac",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "<img src=\"prompt_engeneering.jpeg\" alt=\"Logo Python\" width=\"1800\" height=\"2800\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fd407a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from typing import List, Dict, Any, Optional\n",
    "from pypdf import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import cohere\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "class RAGWithCohereRerank:\n",
    "    \"\"\"\n",
    "    Classe pour exÃ©cuter un pipeline RAG complet avec reranking Cohere.\n",
    "    \n",
    "    FonctionnalitÃ©s :\n",
    "    - Chargement et chunking d'un PDF\n",
    "    - CrÃ©ation d'un index FAISS\n",
    "    - Reranking des rÃ©sultats avec Cohere\n",
    "    - GÃ©nÃ©ration de rÃ©ponse avec OpenAI GPT\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pdf_path: str,\n",
    "        cohere_api_key: str,\n",
    "        openai_api_key: str,\n",
    "        model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        rerank_model: str = \"rerank-multilingual-v3.0\",\n",
    "        gpt_model: str = \"gpt-4o-mini\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialisation du pipeline RAG.\n",
    "        \"\"\"\n",
    "        self.pdf_path = pdf_path\n",
    "        self.cohere_api_key = cohere_api_key\n",
    "        self.openai_api_key = openai_api_key\n",
    "        self.model_name = model_name\n",
    "        self.rerank_model = rerank_model\n",
    "        self.gpt_model = gpt_model\n",
    "\n",
    "        # Initialiser les clients\n",
    "        self.cohere_client = cohere.Client(cohere_api_key)\n",
    "        self.openai_client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "        # Variables internes\n",
    "        self.chunks = None\n",
    "        self.faiss_index = None\n",
    "\n",
    "        # ğŸ‘‡ Charger et dÃ©couper le PDF\n",
    "        self._load_and_split_pdf()\n",
    "        # ğŸ‘‡ Construire l'index FAISS\n",
    "        self._build_index()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _load_and_split_pdf(self):\n",
    "        \"\"\"Charge et dÃ©coupe le PDF en chunks.\"\"\"\n",
    "        reader = PdfReader(self.pdf_path)\n",
    "        full_text = \"\"\n",
    "        for i, page in enumerate(reader.pages):\n",
    "            text = page.extract_text() or \"\"\n",
    "            full_text += text + f\"\\n\\n[PAGE {i+1}]\\n\\n\"\n",
    "\n",
    "        doc = Document(\n",
    "            page_content=full_text.strip().replace('. .', ''),\n",
    "            metadata={\"source\": self.pdf_path}\n",
    "        )\n",
    "\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=100,\n",
    "            separators=['.'],\n",
    "            keep_separator=True\n",
    "        )\n",
    "\n",
    "        self.chunks = text_splitter.split_documents([doc])\n",
    "        return self.chunks\n",
    "\n",
    "    def _build_index(self):\n",
    "        \"\"\"Construit l'index FAISS Ã  partir des chunks.\"\"\"\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=self.model_name)\n",
    "        self.faiss_index = FAISS.from_documents(self.chunks, embeddings)\n",
    "        print(f\"[âœ…] Index FAISS construit avec {len(self.chunks)} chunks.\")\n",
    "\n",
    "    def _rerank_with_cohere(self, query: str, faiss_results: List[Any], top_n: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Rerank les rÃ©sultats FAISS avec Cohere.\n",
    "        \"\"\"\n",
    "        if not faiss_results:\n",
    "            return []\n",
    "\n",
    "        documents = []\n",
    "        for res in faiss_results:\n",
    "            text = getattr(res, \"page_content\", None) or getattr(res, \"content\", None) or str(res)\n",
    "            documents.append(text)\n",
    "\n",
    "        try:\n",
    "            response = self.cohere_client.rerank(\n",
    "                model=self.rerank_model,\n",
    "                query=query,\n",
    "                documents=documents,\n",
    "                top_n=min(top_n, len(documents))\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"[âš ï¸ Warning] Cohere rerank failed: {e}\")\n",
    "            return [\n",
    "                {\"doc\": faiss_results[i], \"original_index\": i, \"score\": None}\n",
    "                for i in range(min(top_n, len(faiss_results)))\n",
    "            ]\n",
    "\n",
    "        results_list = getattr(response, \"results\", None) or response\n",
    "\n",
    "        reranked = []\n",
    "        for item in results_list:\n",
    "            idx = getattr(item, \"index\", None)\n",
    "            score = getattr(item, \"relevance_score\", None) or getattr(item, \"score\", None)\n",
    "\n",
    "            if idx is None or idx >= len(faiss_results):\n",
    "                continue\n",
    "\n",
    "            reranked.append({\n",
    "                \"doc\": faiss_results[idx],\n",
    "                \"original_index\": idx,\n",
    "                \"score\": float(score) if score is not None else None\n",
    "            })\n",
    "\n",
    "        return reranked\n",
    "\n",
    "    def _generate_answer_with_gpt(self, query: str, reranked_results: List[Dict[str, Any]]) -> str:\n",
    "        \"\"\"\n",
    "        GÃ©nÃ¨re une rÃ©ponse avec GPT Ã  partir des rÃ©sultats rerankÃ©s.\n",
    "        \"\"\"\n",
    "        if not reranked_results:\n",
    "            return \"Aucun rÃ©sultat disponible pour rÃ©pondre Ã  votre question.\"\n",
    "\n",
    "        context_parts = []\n",
    "        for i, item in enumerate(reranked_results, 1):\n",
    "            doc = item[\"doc\"]\n",
    "            content = getattr(doc, \"page_content\", \"\") or getattr(doc, \"content\", \"\")\n",
    "            score = item.get(\"score\")\n",
    "            original_idx = item.get(\"original_index\", \"??\")\n",
    "\n",
    "            score_str = f\"{score:.4f}\" if score is not None else \"N/A\"\n",
    "            context_parts.append(\n",
    "                f\"[Document {i} (Orig. idx: {original_idx}) - Score: {score_str}]\\n{content.strip()}\"\n",
    "            )\n",
    "\n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "        system_prompt = (\n",
    "            \"Tu es un assistant intelligent qui rÃ©pond aux questions en t'appuyant \"\n",
    "            \"sur des extraits de documents fournis. Sois clair, concis, et prÃ©cis. \"\n",
    "            \"Si tu ne trouves pas d'information pertinente, dis-le honnÃªtement.\"\n",
    "        )\n",
    "\n",
    "        user_prompt = f\"\"\"\n",
    "Question : {query}\n",
    "\n",
    "Contexte (documents pertinents) :\n",
    "{context}\n",
    "\n",
    "RÃ©ponds de maniÃ¨re complÃ¨te, structurÃ©e, et adaptÃ©e Ã  la question.\n",
    "\"\"\"\n",
    "\n",
    "        try:\n",
    "            completion = self.openai_client.chat.completions.create(\n",
    "                model=self.gpt_model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                max_tokens=500,\n",
    "                temperature=0.3,\n",
    "                n=1\n",
    "            )\n",
    "\n",
    "            answer = completion.choices[0].message.content.strip()\n",
    "            return answer\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[âš ï¸ Error] Ã‰chec de gÃ©nÃ©ration avec GPT : {e}\")\n",
    "            return \"DÃ©solÃ©, je n'ai pas pu gÃ©nÃ©rer de rÃ©ponse pour le moment.\"\n",
    "\n",
    "    def ask(self, query: str, k_retrieve: int = 10, k_rerank: int = 5) -> str:\n",
    "        \"\"\"\n",
    "        RÃ©pond Ã  une question en utilisant le pipeline RAG complet.\n",
    "        \n",
    "        Args:\n",
    "            query (str): La question utilisateur.\n",
    "            k_retrieve (int): Nombre de rÃ©sultats Ã  rÃ©cupÃ©rer depuis FAISS.\n",
    "            k_rerank (int): Nombre de rÃ©sultats Ã  reranker et retourner.\n",
    "\n",
    "        Returns:\n",
    "            str: RÃ©ponse gÃ©nÃ©rÃ©e par GPT.\n",
    "        \"\"\"\n",
    "        print(f\"[ğŸ”] Recherche initiale avec FAISS (k={k_retrieve})...\")\n",
    "        faiss_results = self.faiss_index.similarity_search(query, k=k_retrieve)\n",
    "\n",
    "        print(f\"[ğŸ”„] Reranking avec Cohere (top {k_rerank})...\")\n",
    "        reranked = self._rerank_with_cohere(query, faiss_results, top_n=k_rerank)\n",
    "\n",
    "        print(f\"[ğŸ¤–] GÃ©nÃ©ration de rÃ©ponse avec {self.gpt_model}...\")\n",
    "        final_answer = self._generate_answer_with_gpt(query, reranked)\n",
    "\n",
    "        return final_answer\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bab54ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[âœ…] Index FAISS construit avec 24 chunks.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "PDF_PATH = \"embedding_course_by_koulou.pdf\"  # ou ton fichier PDF\n",
    "\n",
    "# Instancier le pipeline\n",
    "rag = RAGWithCohereRerank(\n",
    "    pdf_path=PDF_PATH,\n",
    "    cohere_api_key=COHERE_API_KEY,\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    rerank_model=\"rerank-multilingual-v3.0\"  # âœ… ModÃ¨le actuel\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c7a5851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ğŸ”] Recherche initiale avec FAISS (k=10)...\n",
      "[ğŸ”„] Reranking avec Cohere (top 5)...\n",
      "[ğŸ¤–] GÃ©nÃ©ration de rÃ©ponse avec gpt-4o-mini...\n",
      "\n",
      "=== RÃ©ponse finale gÃ©nÃ©rÃ©e par le pipeline RAG ===\n",
      "\n",
      "FAISS (Facebook AI Similarity Search) et ChromaDB sont deux outils utilisÃ©s pour la recherche par similaritÃ©, mais ils diffÃ¨rent par leur conception et leur utilisation.\n",
      "\n",
      "### FAISS\n",
      "- **Type** : BibliothÃ¨que bas niveau.\n",
      "- **Optimisation** : SpÃ©cifiquement conÃ§ue pour la recherche par similaritÃ© dense et le clustering.\n",
      "- **Technologie** : Utilise des techniques d'indexation avancÃ©es, comme l'indexation IVF (Inverted File).\n",
      "- **Public cible** : DestinÃ©e aux dÃ©veloppeurs qui ont besoin d'une solution robuste et performante pour des applications nÃ©cessitant une recherche de haute performance sur de grands ensembles de donnÃ©es.\n",
      "\n",
      "### ChromaDB\n",
      "- **Type** : Solution open-source.\n",
      "- **ConvivialitÃ©** : ConÃ§ue pour Ãªtre facile Ã  utiliser, particuliÃ¨rement adaptÃ©e aux dÃ©veloppeurs.\n",
      "- **Utilisation** : IdÃ©ale pour des applications lÃ©gÃ¨res et le prototypage, notamment dans le contexte des modÃ¨les de langage massifs (LLM).\n",
      "- **FonctionnalitÃ©s** : Permet de gÃ©rer des collections de documents et d'effectuer des recherches sÃ©mantiques, facilitant ainsi l'intÃ©gration dans des projets de dÃ©veloppement.\n",
      "\n",
      "### Conclusion\n",
      "En rÃ©sumÃ©, FAISS est une bibliothÃ¨que performante pour des applications nÃ©cessitant une recherche par similaritÃ© Ã  grande Ã©chelle, tandis que ChromaDB se concentre sur la facilitÃ© d'utilisation et le prototypage rapide pour des applications moins exigeantes.\n"
     ]
    }
   ],
   "source": [
    "# Poser une question\n",
    "question = \"Explique-moi la diffÃ©rence entre FAISS et ChromaDB\"\n",
    "response = rag.ask(question, k_retrieve=10, k_rerank=5)\n",
    "print(\"\\n=== RÃ©ponse finale gÃ©nÃ©rÃ©e par le pipeline RAG ===\\n\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
