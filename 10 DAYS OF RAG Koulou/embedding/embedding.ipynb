{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d4aa8a",
   "metadata": {},
   "source": [
    "## BASE DE DONNEES VECTORIELLE ET EMBEDING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc29c594",
   "metadata": {},
   "source": [
    "## ETAPE 1 CHUNKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1c8db05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def load_and_split_pdf_simple(pdf_path: str):\n",
    "    \"\"\"\n",
    "    Charge tout le PDF en un seul Document, puis le d√©coupe en chunks\n",
    "    avec chunk_size=1000 et chunk_overlap=100 (en caract√®res).\n",
    "    \"\"\"\n",
    "    # 1. Extraire tout le texte du PDF\n",
    "    reader = PdfReader(pdf_path)\n",
    "    full_text = \"\"\n",
    "    for i, page in enumerate(reader.pages):\n",
    "        text = page.extract_text() or \"\"\n",
    "        full_text += text + f\"\\n\\n[PAGE {i+1}]\\n\\n\"  # Ajout de marqueur de page\n",
    "\n",
    "    # 2. Cr√©er un seul Document\n",
    "    doc = Document(page_content=full_text.strip().replace('. .',''), metadata={\"source\": pdf_path})\n",
    "\n",
    "    # 3. D√©couper avec RecursiveCharacterTextSplitter (par d√©faut : len = caract√®res)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "        separators=['.'],\n",
    "        keep_separator=True  # Garde les s√©parateurs pour √©viter de couper au milieu d'une phrase\n",
    "    )\n",
    "\n",
    "    # 4. Splitter\n",
    "    chunks = text_splitter.split_documents([doc])\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Utilisation\n",
    "pdf_chunks = load_and_split_pdf_simple(\"embedding_course_by_koulou.pdf\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d58bb5c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='L‚ÄôINTELLIGENCE ARTIFICIELLE\\nDes fondements th√©oriques aux bases de donn√©es vectorielles\\nAuteur : KOULOU crepin\\nD√©partement de Data Science\\n8 janvier 2026\\n\\n[PAGE 1]\\n\\nR√©sum√©\\nCe rapport pr√©sente une analyse approfondie de l‚Äô√©tat actuel de l‚ÄôIntelligence ArtiÔ¨Åcielle.\\nNous explorerons d‚Äôabord les fondements math√©matiques de l‚Äôapprentissage automatique\\n(Machine Learning) et des r√©seaux de neurones profonds (Deep Learning). Une attention\\nparticuli√®re sera port√©e aux d√©veloppements r√©cents concernant les Grands Mod√®les de\\nLangage (LLM) et l‚Äôinfrastructure critique qui les soutient, notamment les bases de don-\\nn√©es vectorielles (FAISS, ChromaDB, Qdrant) et les m√©canismes de similarit√©. EnÔ¨Ån, nous\\naborderons les d√©Ô¨Ås √©thiques et les perspectives d‚Äôavenir.\\n\\n[PAGE 2]\\n\\nTable des mati√®res\\n1 Introduction 2\\n1.1 Contexte Historique                . 2\\n1.2 D√©Ô¨Ånitions et Typologie               2\\n2 Fondements Math√©matiques du Machine Learning 4\\n2.1 L‚ÄôApprentissage Supervis√©              . 4\\n2', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='.1 L‚ÄôApprentissage Supervis√©              . 4\\n2.2 R√©seaux de Neurones et R√©tropropagation          4\\n3 Deep Learning et Architecture Moderne 6\\n3.1 Les R√©seaux de Neurones Convolutifs (CNN)         . 6\\n3.2 Les Transformers et le M√©canisme d‚ÄôAttention         6\\n4 Infrastructure de l‚ÄôIA : Embeddings et Bases Vectorielles 8\\n4.1 Le Concept d‚ÄôEmbedding (Plongement Lexical)        . 8\\n4.2 Mesure de la Similarit√©               . 8\\n4.3 Les Bases de Donn√©es Vectorielles            . 8\\n4.3.1 Comparatif Technique             . 9\\n4.3.2 Exemple d‚Äôimpl√©mentation (Python)         . 9\\n5 D√©Ô¨Ås √âthiques et Avenir 11\\n5.1 Biais Algorithmiques et √âquit√©             . 11\\n5.2 R√©gulation (AI Act)                11\\n6 Conclusion 12\\n\\n[PAGE 3]\\n\\nChapitre 1\\nIntroduction\\nL‚ÄôIntelligence ArtiÔ¨Åcielle (IA) a cess√© d‚Äô√™tre un concept de science-Ô¨Åction pour devenir\\nune force motrice de l‚Äô√©conomie mondiale.\\n1.1 Contexte Historique\\nDepuis le test de Turing jusqu‚Äô√† l‚Äôav√®nement de ChatGPT, l‚Äô√©volution a √©t√© exponen-\\ntielle', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Les premiers syst√®mes experts ont laiss√© place aux r√©seaux de neurones. L‚Äôintelli-\\ngence artiÔ¨Åcielle est un domaine en pleine expansion qui transforme profond√©ment notre\\nsoci√©t√©. Les algorithmes d‚Äôapprentissage automatique permettent aux ordinateurs d‚Äôap-\\nprendre √† partir de donn√©es sans √™tre explicitement programm√©s pour chaque t√¢che. Dans\\nce contexte, les r√©seaux de neurones profonds et les bases de donn√©es vectorielles jouent\\nun r√¥le crucial pour traiter des informations non structur√©es. Ce paragraphe sert de rem-\\nplissage pour simuler le volume de texte du rapport Ô¨Ånal, permettant d‚Äôappr√©cier la mise\\nen page sans √™tre distrait par du latin. Les avanc√©es technologiques r√©centes, notamment\\nles architectures Transformers, ont ouvert la voie aux mod√®les de langage massifs (LLM).\\nL‚Äôintelligence artiÔ¨Åcielle est un domaine en pleine expansion qui transforme profond√©-\\nment notre soci√©t√©', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Les algorithmes d‚Äôapprentissage automatique permettent aux ordina-\\nteurs d‚Äôapprendre √† partir de donn√©es sans √™tre explicitement programm√©s pour chaque\\nt√¢che. Dans ce contexte, les r√©seaux de neurones profonds et les bases de donn√©es vecto-\\nrielles jouent un r√¥le crucial pour traiter des informations non structur√©es. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport Ô¨Ånal, permettant d‚Äôappr√©-\\ncier la mise en page sans √™tre distrait par du latin. Les avanc√©es technologiques r√©centes,\\nnotamment les architectures Transformers, ont ouvert la voie aux mod√®les de langage\\nmassifs (LLM).\\n1.2 D√©Ô¨Ånitions et Typologie\\nIl convient de distinguer l‚ÄôIA faible (ANI), sp√©cialis√©e dans une t√¢che, de l‚ÄôIA forte\\n(AGI), hypoth√©tique et capable de g√©n√©ralisation humaine. L‚Äôintelligence artiÔ¨Åcielle est un\\ndomaine en pleine expansion qui transforme profond√©ment notre soci√©t√©', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Les algorithmes\\nd‚Äôapprentissage automatique permettent aux ordinateurs d‚Äôapprendre √† partir de donn√©es\\nsans √™tre explicitement programm√©s pour chaque t√¢che. Dans ce contexte, les r√©seaux de\\nneuronesprofondsetlesbasesdedonn√©esvectoriellesjouentunr√¥lecrucialpourtraiterdes\\n2\\n\\n[PAGE 4]\\n\\n√âtat de l‚Äôart Rapport IA 2024\\ninformations non structur√©es. Ce paragraphe sert de remplissage pour simuler le volume\\nde texte du rapport Ô¨Ånal, permettant d‚Äôappr√©cier la mise en page sans √™tre distrait par\\ndu latin. Les avanc√©es technologiques r√©centes, notamment les architectures Transformers,\\nont ouvert la voie aux mod√®les de langage massifs (LLM).\\n3\\n\\n[PAGE 5]\\n\\nChapitre 2\\nFondements Math√©matiques du\\nMachine Learning\\nL‚ÄôIA repose avant tout sur l‚Äôoptimisation math√©matique et les statistiques avanc√©es.\\n2.1 L‚ÄôApprentissage Supervis√©\\nDans un cadre de r√©gression ou de classiÔ¨Åcation, nous cherchons √† minimiser une\\nfonction de co√ªt (Loss Function).\\nSoit un jeu de donn√©esD = {(xi,yi)}n\\ni=1', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='.\\nSoit un jeu de donn√©esD = {(xi,yi)}n\\ni=1. Pour un mod√®le lin√©airef(x) = wT x+ b,\\nnous cherchons √† minimiser l‚Äôerreur quadratique moyenne (MSE) :\\nJ(w,b) = 1\\nn\\nn‚àë\\ni=1\\n(yi ‚àí(wT xi + b))2 (2.1)\\nL‚Äôalgorithme de descente de gradient permet de mettre √† jour les poidsw de mani√®re\\nit√©rative :\\nw‚Üêw‚àíŒ±‚àáwJ(w,b) (2.2)\\no√π Œ± repr√©sente le taux d‚Äôapprentissage (learning rate), un hyperparam√®tre crucial.\\n2.2 R√©seaux de Neurones et R√©tropropagation\\nUn neurone artiÔ¨Åciel applique une fonction d‚Äôactivation non lin√©aireœÉ (comme ReLU\\nou Sigmoid) :\\na[l] = œÉ(W[l]a[l‚àí1] + b[l]) (2.3)\\nL‚Äôintelligence artiÔ¨Åcielle est un domaine en pleine expansion qui transforme profond√©-\\nment notre soci√©t√©. Les algorithmes d‚Äôapprentissage automatique permettent aux ordina-\\nteurs d‚Äôapprendre √† partir de donn√©es sans √™tre explicitement programm√©s pour chaque\\nt√¢che. Dans ce contexte, les r√©seaux de neurones profonds et les bases de donn√©es vecto-\\nrielles jouent un r√¥le crucial pour traiter des informations non structur√©es', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport Ô¨Ånal, permettant d‚Äôappr√©-\\ncier la mise en page sans √™tre distrait par du latin. Les avanc√©es technologiques r√©centes,\\n4\\n\\n[PAGE 6]\\n\\n√âtat de l‚Äôart Rapport IA 2024\\nnotamment les architectures Transformers, ont ouvert la voie aux mod√®les de langage\\nmassifs (LLM).\\nL‚Äôintelligence artiÔ¨Åcielle est un domaine en pleine expansion qui transforme profond√©-\\nment notre soci√©t√©. Les algorithmes d‚Äôapprentissage automatique permettent aux ordina-\\nteurs d‚Äôapprendre √† partir de donn√©es sans √™tre explicitement programm√©s pour chaque\\nt√¢che. Dans ce contexte, les r√©seaux de neurones profonds et les bases de donn√©es vecto-\\nrielles jouent un r√¥le crucial pour traiter des informations non structur√©es. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport Ô¨Ånal, permettant d‚Äôappr√©-\\ncier la mise en page sans √™tre distrait par du latin', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Les avanc√©es technologiques r√©centes,\\nnotamment les architectures Transformers, ont ouvert la voie aux mod√®les de langage\\nmassifs (LLM).\\nL‚Äôintelligence artiÔ¨Åcielle est un domaine en pleine expansion qui transforme profond√©-\\nment notre soci√©t√©. Les algorithmes d‚Äôapprentissage automatique permettent aux ordina-\\nteurs d‚Äôapprendre √† partir de donn√©es sans √™tre explicitement programm√©s pour chaque\\nt√¢che. Dans ce contexte, les r√©seaux de neurones profonds et les bases de donn√©es vecto-\\nrielles jouent un r√¥le crucial pour traiter des informations non structur√©es. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport Ô¨Ånal, permettant d‚Äôappr√©-\\ncier la mise en page sans √™tre distrait par du latin. Les avanc√©es technologiques r√©centes,\\nnotamment les architectures Transformers, ont ouvert la voie aux mod√®les de langage\\nmassifs (LLM).\\nL‚Äôintelligence artiÔ¨Åcielle est un domaine en pleine expansion qui transforme profond√©-\\nment notre soci√©t√©', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Les algorithmes d‚Äôapprentissage automatique permettent aux ordina-\\nteurs d‚Äôapprendre √† partir de donn√©es sans √™tre explicitement programm√©s pour chaque\\nt√¢che. Dans ce contexte, les r√©seaux de neurones profonds et les bases de donn√©es vecto-\\nrielles jouent un r√¥le crucial pour traiter des informations non structur√©es. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport Ô¨Ånal, permettant d‚Äôappr√©-\\ncier la mise en page sans √™tre distrait par du latin. Les avanc√©es technologiques r√©centes,\\nnotamment les architectures Transformers, ont ouvert la voie aux mod√®les de langage\\nmassifs (LLM).\\n5\\n\\n[PAGE 7]\\n\\nChapitre 3\\nDeep Learning et Architecture Moderne\\n3.1 Les R√©seaux de Neurones Convolutifs (CNN)\\nUtilis√©s principalement en vision par ordinateur pour la reconnaissance d‚Äôimages. L‚Äôin-\\ntelligence artiÔ¨Åcielle est un domaine en pleine expansion qui transforme profond√©ment\\nnotre soci√©t√©', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Les algorithmes d‚Äôapprentissage automatique permettent aux ordinateurs\\nd‚Äôapprendre √† partir de donn√©es sans √™tre explicitement programm√©s pour chaque t√¢che.\\nDans ce contexte, les r√©seaux de neurones profonds et les bases de donn√©es vectorielles\\njouent un r√¥le crucial pour traiter des informations non structur√©es. Ce paragraphe sert\\nde remplissage pour simuler le volume de texte du rapport Ô¨Ånal, permettant d‚Äôappr√©cier la\\nmise en page sans √™tre distrait par du latin. Les avanc√©es technologiques r√©centes, notam-\\nment les architectures Transformers, ont ouvert la voie aux mod√®les de langage massifs\\n(LLM).\\n3.2 Les Transformers et le M√©canisme d‚ÄôAttention\\nL‚Äôinnovation majeure de l‚Äôarticle fondateur de 2017 a r√©volutionn√© le traitement du\\nlangage naturel (NLP). L‚Äôattention se calcule ainsi :\\nAttention(Q,K,V ) =softmax\\n(QKT\\n‚àödk\\n)\\nV (3.1)\\nCette architecture permet de parall√©liser les calculs, contrairement aux r√©seaux r√©cur-\\nrents (RNN) pr√©c√©dents', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. L‚Äôintelligence artiÔ¨Åcielle est un domaine en pleine expansion\\nqui transforme profond√©ment notre soci√©t√©. Les algorithmes d‚Äôapprentissage automatique\\npermettent aux ordinateurs d‚Äôapprendre √† partir de donn√©es sans √™tre explicitement pro-\\ngramm√©s pour chaque t√¢che. Dans ce contexte, les r√©seaux de neurones profonds et les\\nbases de donn√©es vectorielles jouent un r√¥le crucial pour traiter des informations non\\nstructur√©es. Ce paragraphe sert de remplissage pour simuler le volume de texte du rap-\\nport Ô¨Ånal, permettant d‚Äôappr√©cier la mise en page sans √™tre distrait par du latin. Les\\navanc√©es technologiques r√©centes, notamment les architectures Transformers, ont ouvert\\nla voie aux mod√®les de langage massifs (LLM).\\nL‚Äôintelligence artiÔ¨Åcielle est un domaine en pleine expansion qui transforme profond√©-\\nment notre soci√©t√©', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Les algorithmes d‚Äôapprentissage automatique permettent aux ordina-\\nteurs d‚Äôapprendre √† partir de donn√©es sans √™tre explicitement programm√©s pour chaque\\n6\\n\\n[PAGE 8]\\n\\n√âtat de l‚Äôart Rapport IA 2024\\nt√¢che. Dans ce contexte, les r√©seaux de neurones profonds et les bases de donn√©es vecto-\\nrielles jouent un r√¥le crucial pour traiter des informations non structur√©es. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport Ô¨Ånal, permettant d‚Äôappr√©-\\ncier la mise en page sans √™tre distrait par du latin. Les avanc√©es technologiques r√©centes,\\nnotamment les architectures Transformers, ont ouvert la voie aux mod√®les de langage\\nmassifs (LLM).\\nL‚Äôintelligence artiÔ¨Åcielle est un domaine en pleine expansion qui transforme profond√©-\\nment notre soci√©t√©. Les algorithmes d‚Äôapprentissage automatique permettent aux ordina-\\nteurs d‚Äôapprendre √† partir de donn√©es sans √™tre explicitement programm√©s pour chaque\\nt√¢che', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Dans ce contexte, les r√©seaux de neurones profonds et les bases de donn√©es vecto-\\nrielles jouent un r√¥le crucial pour traiter des informations non structur√©es. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport Ô¨Ånal, permettant d‚Äôappr√©-\\ncier la mise en page sans √™tre distrait par du latin. Les avanc√©es technologiques r√©centes,\\nnotamment les architectures Transformers, ont ouvert la voie aux mod√®les de langage\\nmassifs (LLM).\\nL‚Äôintelligence artiÔ¨Åcielle est un domaine en pleine expansion qui transforme profond√©-\\nment notre soci√©t√©. Les algorithmes d‚Äôapprentissage automatique permettent aux ordina-\\nteurs d‚Äôapprendre √† partir de donn√©es sans √™tre explicitement programm√©s pour chaque\\nt√¢che. Dans ce contexte, les r√©seaux de neurones profonds et les bases de donn√©es vecto-\\nrielles jouent un r√¥le crucial pour traiter des informations non structur√©es', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport Ô¨Ånal, permettant d‚Äôappr√©-\\ncier la mise en page sans √™tre distrait par du latin. Les avanc√©es technologiques r√©centes,\\nnotamment les architectures Transformers, ont ouvert la voie aux mod√®les de langage\\nmassifs (LLM).\\n7\\n\\n[PAGE 9]\\n\\nChapitre 4\\nInfrastructure de l‚ÄôIA : Embeddings et\\nBases Vectorielles\\nC‚Äôest ici que r√©side le c≈ìur des syst√®mes RAG (G√©n√©ration Augment√©e par la R√©cu-\\np√©ration) modernes.\\n4.1 Le Concept d‚ÄôEmbedding (Plongement Lexical)\\nLes embeddings transforment des donn√©es non structur√©es (texte, image, son) en vec-\\nteurs denses de dimensiond. L‚Äôintelligence artiÔ¨Åcielle est un domaine en pleine expansion\\nqui transforme profond√©ment notre soci√©t√©. Les algorithmes d‚Äôapprentissage automatique\\npermettent aux ordinateurs d‚Äôapprendre √† partir de donn√©es sans √™tre explicitement pro-\\ngramm√©s pour chaque t√¢che', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Dans ce contexte, les r√©seaux de neurones profonds et les\\nbases de donn√©es vectorielles jouent un r√¥le crucial pour traiter des informations non\\nstructur√©es. Ce paragraphe sert de remplissage pour simuler le volume de texte du rap-\\nport Ô¨Ånal, permettant d‚Äôappr√©cier la mise en page sans √™tre distrait par du latin. Les\\navanc√©es technologiques r√©centes, notamment les architectures Transformers, ont ouvert\\nla voie aux mod√®les de langage massifs (LLM).\\n4.2 Mesure de la Similarit√©\\nComme vu pr√©c√©demment, la similarit√© cosinus est la m√©trique standard pour √©valuer\\nla proximit√© s√©mantique :\\nsim(A,B) = cos(Œ∏) = A¬∑B\\n‚à•A‚à•‚à•B‚à•=\\n‚àën\\ni=1 AiBi‚àö‚àën\\ni=1 A2\\ni\\n‚àö‚àën\\ni=1 B2\\ni\\n(4.1)\\n4.3 Les Bases de Donn√©es Vectorielles\\nPour g√©rer des millions de vecteurs, des solutions sp√©cialis√©es sont n√©cessaires. Les\\nbases de donn√©es relationnelles classiques (SQL) ne sont pas optimis√©es pour ce type de\\ncalcul.\\n8\\n\\n[PAGE 10]\\n\\n√âtat de l‚Äôart Rapport IA 2024\\n4.3', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='.\\n8\\n\\n[PAGE 10]\\n\\n√âtat de l‚Äôart Rapport IA 2024\\n4.3.1 Comparatif Technique\\n‚Äî FAISS (Meta): Biblioth√®que bas niveau, optimis√©e pour la recherche par simila-\\nrit√© dense et le clustering. Utilise l‚Äôindexation IVF.\\n‚Äî ChromaDB : Solution open-source conviviale pour les d√©veloppeurs, id√©ale pour\\nles applications LLM l√©g√®res et le prototypage.\\n‚Äî Pinecone / Qdrant: Solutions g√©r√©es (SaaS) ou robustes (Rust) pour la produc-\\ntion √† grande √©chelle.\\n4.3.2 Exemple d‚Äôimpl√©mentation (Python)\\nVoici comment on initialise une recherche simple avec la librairie ChromaDB :\\n1 import chromadb\\n2\\n3 # Initialisation du client (en m√© moire pour l‚Äô exemple )\\n4 client = chromadb . Client ()\\n5 collection = client . create_collection (\" mon_dataset_ia \")\\n6\\n7 # Ajout de documents ( texte brut + m√© tadonn √©es)\\n8 collection', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. add (\\n9 documents =[\"L‚ÄôIA vectorielle est puissante \", \"Le chat mange des\\ncroquettes \"],\\n10 metadatas =[{ \" source \": \" cours \"}, {\" source \": \" vie \"}] ,\\n11 ids =[\" id1 \", \" id2 \"]\\n12 )\\n13\\n14 # Recherche s√© mantique (Le plus proche voisin )\\n15 results = collection . query (\\n16 query_texts =[\" Base de donn √©es vecteur \"],\\n17 n_results =1\\n18 )\\n19\\nListing 4.1 ‚Äì Exemple de code ChromaDB pour la recherche s√©mantique\\nL‚Äôintelligence artiÔ¨Åcielle est un domaine en pleine expansion qui transforme profond√©-\\nment notre soci√©t√©. Les algorithmes d‚Äôapprentissage automatique permettent aux ordina-\\nteurs d‚Äôapprendre √† partir de donn√©es sans √™tre explicitement programm√©s pour chaque\\nt√¢che. Dans ce contexte, les r√©seaux de neurones profonds et les bases de donn√©es vecto-\\nrielles jouent un r√¥le crucial pour traiter des informations non structur√©es. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport Ô¨Ånal, permettant d‚Äôappr√©-\\ncier la mise en page sans √™tre distrait par du latin', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Les avanc√©es technologiques r√©centes,\\nnotamment les architectures Transformers, ont ouvert la voie aux mod√®les de langage\\nmassifs (LLM).\\nL‚Äôintelligence artiÔ¨Åcielle est un domaine en pleine expansion qui transforme profond√©-\\nment notre soci√©t√©. Les algorithmes d‚Äôapprentissage automatique permettent aux ordina-\\nteurs d‚Äôapprendre √† partir de donn√©es sans √™tre explicitement programm√©s pour chaque\\nt√¢che. Dans ce contexte, les r√©seaux de neurones profonds et les bases de donn√©es vecto-\\nrielles jouent un r√¥le crucial pour traiter des informations non structur√©es. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport Ô¨Ånal, permettant d‚Äôappr√©-\\ncier la mise en page sans √™tre distrait par du latin. Les avanc√©es technologiques r√©centes,\\nnotamment les architectures Transformers, ont ouvert la voie aux mod√®les de langage\\nmassifs (LLM)', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='.\\n9\\n\\n[PAGE 11]\\n\\n√âtat de l‚Äôart Rapport IA 2024\\nL‚Äôintelligence artiÔ¨Åcielle est un domaine en pleine expansion qui transforme profond√©-\\nment notre soci√©t√©. Les algorithmes d‚Äôapprentissage automatique permettent aux ordina-\\nteurs d‚Äôapprendre √† partir de donn√©es sans √™tre explicitement programm√©s pour chaque\\nt√¢che. Dans ce contexte, les r√©seaux de neurones profonds et les bases de donn√©es vecto-\\nrielles jouent un r√¥le crucial pour traiter des informations non structur√©es. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport Ô¨Ånal, permettant d‚Äôappr√©-\\ncier la mise en page sans √™tre distrait par du latin. Les avanc√©es technologiques r√©centes,\\nnotamment les architectures Transformers, ont ouvert la voie aux mod√®les de langage\\nmassifs (LLM).\\nL‚Äôintelligence artiÔ¨Åcielle est un domaine en pleine expansion qui transforme profond√©-\\nment notre soci√©t√©', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Les algorithmes d‚Äôapprentissage automatique permettent aux ordina-\\nteurs d‚Äôapprendre √† partir de donn√©es sans √™tre explicitement programm√©s pour chaque\\nt√¢che. Dans ce contexte, les r√©seaux de neurones profonds et les bases de donn√©es vecto-\\nrielles jouent un r√¥le crucial pour traiter des informations non structur√©es. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport Ô¨Ånal, permettant d‚Äôappr√©-\\ncier la mise en page sans √™tre distrait par du latin. Les avanc√©es technologiques r√©centes,\\nnotamment les architectures Transformers, ont ouvert la voie aux mod√®les de langage\\nmassifs (LLM).\\n10\\n\\n[PAGE 12]\\n\\nChapitre 5\\nD√©Ô¨Ås √âthiques et Avenir\\n5.1 Biais Algorithmiques et √âquit√©\\nLes mod√®les d‚ÄôIA peuvent reproduire, voire ampliÔ¨Åer, les biais pr√©sents dans leurs\\ndonn√©es d‚Äôentra√Ænement. L‚Äôintelligence artiÔ¨Åcielle est un domaine en pleine expansion\\nqui transforme profond√©ment notre soci√©t√©', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Les algorithmes d‚Äôapprentissage automatique\\npermettent aux ordinateurs d‚Äôapprendre √† partir de donn√©es sans √™tre explicitement pro-\\ngramm√©s pour chaque t√¢che. Dans ce contexte, les r√©seaux de neurones profonds et les\\nbases de donn√©es vectorielles jouent un r√¥le crucial pour traiter des informations non\\nstructur√©es. Ce paragraphe sert de remplissage pour simuler le volume de texte du rap-\\nport Ô¨Ånal, permettant d‚Äôappr√©cier la mise en page sans √™tre distrait par du latin. Les\\navanc√©es technologiques r√©centes, notamment les architectures Transformers, ont ouvert\\nla voie aux mod√®les de langage massifs (LLM).\\n5.2 R√©gulation (AI Act)\\nL‚ÄôUnion Europ√©enne et d‚Äôautres instances travaillent sur des cadres l√©gaux. L‚Äôintelli-\\ngence artiÔ¨Åcielle est un domaine en pleine expansion qui transforme profond√©ment notre\\nsoci√©t√©. Les algorithmes d‚Äôapprentissage automatique permettent aux ordinateurs d‚Äôap-\\nprendre √† partir de donn√©es sans √™tre explicitement programm√©s pour chaque t√¢che', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Dans\\nce contexte, les r√©seaux de neurones profonds et les bases de donn√©es vectorielles jouent\\nun r√¥le crucial pour traiter des informations non structur√©es. Ce paragraphe sert de rem-\\nplissage pour simuler le volume de texte du rapport Ô¨Ånal, permettant d‚Äôappr√©cier la mise\\nen page sans √™tre distrait par du latin. Les avanc√©es technologiques r√©centes, notamment\\nles architectures Transformers, ont ouvert la voie aux mod√®les de langage massifs (LLM).\\nL‚Äôintelligence artiÔ¨Åcielle est un domaine en pleine expansion qui transforme profond√©-\\nment notre soci√©t√©. Les algorithmes d‚Äôapprentissage automatique permettent aux ordina-\\nteurs d‚Äôapprendre √† partir de donn√©es sans √™tre explicitement programm√©s pour chaque\\nt√¢che. Dans ce contexte, les r√©seaux de neurones profonds et les bases de donn√©es vecto-\\nrielles jouent un r√¥le crucial pour traiter des informations non structur√©es', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport Ô¨Ånal, permettant d‚Äôappr√©-\\ncier la mise en page sans √™tre distrait par du latin. Les avanc√©es technologiques r√©centes,\\nnotamment les architectures Transformers, ont ouvert la voie aux mod√®les de langage\\nmassifs (LLM).\\n11\\n\\n[PAGE 13]\\n\\nChapitre 6\\nConclusion\\nEn conclusion, l‚Äôintelligence artiÔ¨Åcielle traverse une phase de maturation industrielle.\\nL‚Äôint√©gration des bases de donn√©es vectorielles comme m√©moire √† long terme pour les\\nLLM ouvre la voie √† des agents autonomes plus performants et contextuels. L‚Äôintelli-\\ngence artiÔ¨Åcielle est un domaine en pleine expansion qui transforme profond√©ment notre\\nsoci√©t√©. Les algorithmes d‚Äôapprentissage automatique permettent aux ordinateurs d‚Äôap-\\nprendre √† partir de donn√©es sans √™tre explicitement programm√©s pour chaque t√¢che', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Dans\\nce contexte, les r√©seaux de neurones profonds et les bases de donn√©es vectorielles jouent\\nun r√¥le crucial pour traiter des informations non structur√©es. Ce paragraphe sert de rem-\\nplissage pour simuler le volume de texte du rapport Ô¨Ånal, permettant d‚Äôappr√©cier la mise\\nen page sans √™tre distrait par du latin. Les avanc√©es technologiques r√©centes, notamment\\nles architectures Transformers, ont ouvert la voie aux mod√®les de langage massifs (LLM).\\n12\\n\\n[PAGE 14]\\n\\nBibliographie\\n[1] Vaswani, A., et al. (2017).Attention Is All You Need(L‚Äôattention est tout ce dont\\nvous avez besoin). Advances in Neural Information Processing Systems.\\n[2] LeCun, Y., Bengio, Y., & Hinton, G. (2015).Deep learning(L‚Äôapprentissage profond).\\nNature, 521(7553), 436-444.\\n[3] Johnson, J., Douze, M., & J√©gou, H. (2017).Billion-scale similarity search with GPUs\\n(Recherche de similarit√© √† l‚Äô√©chelle du milliard avec GPU). IEEE Transactions on Big\\nData.\\n13\\n\\n[PAGE 15]', metadata={'source': 'embedding_course_by_koulou.pdf'})]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e3c8d4",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"faiss chroma.jpg\" alt=\"Logo Python\" width=\"1500\" height=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e92ee7",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; gap: 20px;\">\n",
    "\n",
    "  <!-- Colonne 1 -->\n",
    "  <div style=\"flex: 1; padding: 15px; border: 2px solid #e65100; border-radius: 10px; background-color: #fff0e0; box-shadow: 2px 2px 8px rgba(0,0,0,0.1); color:#212121;\">\n",
    "  \n",
    "  ## 1. Introduction G√©n√©rale : Les Bases de Donn√©es Vectorielles\n",
    "\n",
    "  **Qu'est-ce que c'est ?**  \n",
    "  Une base de donn√©es vectorielle est un syst√®me optimis√© pour stocker et interroger des *vecteurs* (embeddings).\n",
    "\n",
    "  **Pourquoi en avons-nous besoin ?**  \n",
    "  Les bases traditionnelles (SQL, NoSQL) g√®rent bien les correspondances exactes, mais √©chouent sur la *similarit√©*.  \n",
    "\n",
    "  Avec l‚Äôessor de l‚ÄôIA, les donn√©es complexes (texte, images, audio) sont transform√©es en vecteurs.  \n",
    "  ‚ú® Des concepts proches se retrouvent math√©matiquement voisins dans un espace multidimensionnel.  \n",
    "\n",
    "  üëâ Fonction principale : **Vector Search**.  \n",
    "  Exemple : un vecteur \"chat\" renvoie les images les plus proches d‚Äôun chat, m√™me sans mot-cl√©.  \n",
    "\n",
    "  C‚Äôest le c≈ìur des applications modernes comme le **RAG (Retrieval-Augmented Generation)**, qui permet aux LLM d‚Äôacc√©der √† vos documents.  \n",
    "\n",
    "  </div>\n",
    "\n",
    "  <!-- Colonne 2 -->\n",
    "  <div style=\"flex: 1; padding: 15px; border: 2px solid #e65100; border-radius: 10px; background-color: #fff0e0; box-shadow: 2px 2px 8px rgba(0,0,0,0.1); color:#212121;\">\n",
    "  \n",
    "  ## 2. FAISS et ChromaDB : Solutions orient√©es \"D√©veloppement Local\"\n",
    "\n",
    "  **FAISS (Facebook AI Similarity Search)**  \n",
    "  - Origine : Meta (Facebook AI Research).  \n",
    "  - Nature : Biblioth√®que C++/Python, ultra performante.  \n",
    "  - Forces :  \n",
    "    - üöÄ Recherche sur milliards de vecteurs.  \n",
    "    - ‚ö° Optimisation GPU.  \n",
    "  - Faiblesses :  \n",
    "    - Bas niveau, pas de stockage persistant.  \n",
    "    - Risque de perte si non sauvegard√©.  \n",
    "\n",
    "  **ChromaDB**  \n",
    "  - Origine : Projet Open Source r√©cent.  \n",
    "  - Nature : Base vectorielle simple et rapide.  \n",
    "  - Forces :  \n",
    "    - üéØ Installation facile (`pip install chromadb`).  \n",
    "    - üîÑ Vectorisation int√©gr√©e.  \n",
    "  - Faiblesses :  \n",
    "    - Id√©al pour petits/moyens projets.  \n",
    "    - Moins robuste pour t√©raoctets en production.  \n",
    "\n",
    "  </div>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e1ff90",
   "metadata": {},
   "source": [
    "## FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc849efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def build_faiss_index(chunks):\n",
    "    \"\"\"\n",
    "    Construit un index FAISS √† partir de chunks d√©j√† pr√©par√©s.\n",
    "    Utilise un mod√®le gratuit Hugging Face pour g√©n√©rer les embeddings.\n",
    "    \"\"\"\n",
    "    # 1. Cr√©er les embeddings avec Hugging Face\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"  # mod√®le open-source ou openaiembeddings\n",
    "    )\n",
    "\n",
    "    # 2. Construire l‚Äôindex FAISS\n",
    "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "    return vectorstore\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a413f039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- R√©sultat 1 ---\n",
      ".\n",
      "8\n",
      "\n",
      "[PAGE 10]\n",
      "\n",
      "√âtat de l‚Äôart Rapport IA 2024\n",
      "4.3.1 Comparatif Technique\n",
      "‚Äî FAISS (Meta): Biblioth√®que bas niveau, optimis√©e pour la recherche par simila-\n",
      "rit√© dense et le clustering. Utilise l‚Äôindexation IVF.\n",
      "‚Äî ChromaDB : Solution open-source conviviale pour les d√©veloppeurs, id√©ale pour\n",
      "les applications LLM l√©g√®res et le prototypage.\n",
      "‚Äî Pinecone / Qdrant: Solutions g√©r√©es (SaaS) ou robustes (Rust) pour la produc-\n",
      "tion √† grande √©chelle.\n",
      "4.3.2 Exemple d‚Äôimpl√©mentation (Python)\n",
      "Voici comment on ini ...\n",
      "--- R√©sultat 2 ---\n",
      ". Dans ce contexte, les r√©seaux de neurones profonds et les\n",
      "bases de donn√©es vectorielles jouent un r√¥le crucial pour traiter des informations non\n",
      "structur√©es. Ce paragraphe sert de remplissage pour simuler le volume de texte du rap-\n",
      "port Ô¨Ånal, permettant d‚Äôappr√©cier la mise en page sans √™tre distrait par du latin. Les\n",
      "avanc√©es technologiques r√©centes, notamment les architectures Transformers, ont ouvert\n",
      "la voie aux mod√®les de langage massifs (LLM).\n",
      "4.2 Mesure de la Similarit√©\n",
      "Comme vu pr√©c√©demm ...\n",
      "--- R√©sultat 3 ---\n",
      ". add (\n",
      "9 documents =[\"L‚ÄôIA vectorielle est puissante \", \"Le chat mange des\n",
      "croquettes \"],\n",
      "10 metadatas =[{ \" source \": \" cours \"}, {\" source \": \" vie \"}] ,\n",
      "11 ids =[\" id1 \", \" id2 \"]\n",
      "12 )\n",
      "13\n",
      "14 # Recherche s√© mantique (Le plus proche voisin )\n",
      "15 results = collection . query (\n",
      "16 query_texts =[\" Base de donn √©es vecteur \"],\n",
      "17 n_results =1\n",
      "18 )\n",
      "19\n",
      "Listing 4.1 ‚Äì Exemple de code ChromaDB pour la recherche s√©mantique\n",
      "L‚Äôintelligence artiÔ¨Åcielle est un domaine en pleine expansion qui transforme profo ...\n"
     ]
    }
   ],
   "source": [
    "# Supposons que tu as d√©j√† tes chunks :\n",
    "# chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "faiss_index = build_faiss_index(pdf_chunks)\n",
    "\n",
    "# Faire une requ√™te\n",
    "query = \"Explique-moi la diff√©rence entre FAISS et ChromaDB\"\n",
    "results = faiss_index.similarity_search(query, k=3)\n",
    "\n",
    "# Afficher les r√©sultats\n",
    "for i, res in enumerate(results):\n",
    "    print(f\"--- R√©sultat {i+1} ---\")\n",
    "    print(res.page_content[:500], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d9126d",
   "metadata": {},
   "source": [
    "## CHROMA DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2bc9ec63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def build_chroma_index(chunks, persist_directory=\"chroma_index\"):\n",
    "    \"\"\"\n",
    "    Construit un index Chroma √† partir de chunks d√©j√† pr√©par√©s.\n",
    "    Utilise un mod√®le gratuit Hugging Face pour g√©n√©rer les embeddings.\n",
    "    Les donn√©es sont persist√©es dans un dossier local.\n",
    "    \"\"\"\n",
    "    # 1. Cr√©er les embeddings avec Hugging Face\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"  # mod√®le open-source\n",
    "    )\n",
    "\n",
    "    # 2. Construire l‚Äôindex Chroma\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory  # dossier o√π sauvegarder l‚Äôindex\n",
    "    )\n",
    "\n",
    "    # 3. Sauvegarder l‚Äôindex\n",
    "    vectorstore.persist()\n",
    "\n",
    "    return vectorstore\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "734a61ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- R√©sultat 1 ---\n",
      ".\n",
      "8\n",
      "\n",
      "[PAGE 10]\n",
      "\n",
      "√âtat de l‚Äôart Rapport IA 2024\n",
      "4.3.1 Comparatif Technique\n",
      "‚Äî FAISS (Meta): Biblioth√®que bas niveau, optimis√©e pour la recherche par simila-\n",
      "rit√© dense et le clustering. Utilise l‚Äôindexation IVF.\n",
      "‚Äî ChromaDB : Solution open-source conviviale pour les d√©veloppeurs, id√©ale pour\n",
      "les applications LLM l√©g√®res et le prototypage.\n",
      "‚Äî Pinecone / Qdrant: Solutions g√©r√©es (SaaS) ou robustes (Rust) pour la produc-\n",
      "tion √† grande √©chelle.\n",
      "4.3.2 Exemple d‚Äôimpl√©mentation (Python)\n",
      "Voici comment on ini ...\n",
      "--- R√©sultat 2 ---\n",
      ". Dans ce contexte, les r√©seaux de neurones profonds et les\n",
      "bases de donn√©es vectorielles jouent un r√¥le crucial pour traiter des informations non\n",
      "structur√©es. Ce paragraphe sert de remplissage pour simuler le volume de texte du rap-\n",
      "port Ô¨Ånal, permettant d‚Äôappr√©cier la mise en page sans √™tre distrait par du latin. Les\n",
      "avanc√©es technologiques r√©centes, notamment les architectures Transformers, ont ouvert\n",
      "la voie aux mod√®les de langage massifs (LLM).\n",
      "4.2 Mesure de la Similarit√©\n",
      "Comme vu pr√©c√©demm ...\n",
      "--- R√©sultat 3 ---\n",
      ". add (\n",
      "9 documents =[\"L‚ÄôIA vectorielle est puissante \", \"Le chat mange des\n",
      "croquettes \"],\n",
      "10 metadatas =[{ \" source \": \" cours \"}, {\" source \": \" vie \"}] ,\n",
      "11 ids =[\" id1 \", \" id2 \"]\n",
      "12 )\n",
      "13\n",
      "14 # Recherche s√© mantique (Le plus proche voisin )\n",
      "15 results = collection . query (\n",
      "16 query_texts =[\" Base de donn √©es vecteur \"],\n",
      "17 n_results =1\n",
      "18 )\n",
      "19\n",
      "Listing 4.1 ‚Äì Exemple de code ChromaDB pour la recherche s√©mantique\n",
      "L‚Äôintelligence artiÔ¨Åcielle est un domaine en pleine expansion qui transforme profo ...\n"
     ]
    }
   ],
   "source": [
    "# Supposons que tu as d√©j√† tes chunks :\n",
    "# chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Construire l‚Äôindex\n",
    "chroma_index = build_chroma_index(pdf_chunks)\n",
    "\n",
    "# Faire une requ√™te\n",
    "query = \"Explique-moi la diff√©rence entre FAISS et ChromaDB\"\n",
    "results = chroma_index.similarity_search(query, k=3)\n",
    "\n",
    "# Afficher les r√©sultats\n",
    "for i, res in enumerate(results):\n",
    "    print(f\"--- R√©sultat {i+1} ---\")\n",
    "    print(res.page_content[:500], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ae35eb",
   "metadata": {},
   "source": [
    "# explication du fonctionnement de l algo de recherche et mesure de similarite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcac3bfd",
   "metadata": {},
   "source": [
    "## <img src=\"cosine_similarity.jpg\" alt=\"Logo Python\" width=\"1500\" height=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1959a7",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; gap: 20px;\">\n",
    "\n",
    "  <!-- Colonne 1 : Math√©matique -->\n",
    "  <div style=\"flex: 1; padding: 15px; border: 2px solid #e65100; border-radius: 10px; background-color: #fff0e0; box-shadow: 2px 2px 8px rgba(0,0,0,0.1); color:#212121;\">\n",
    "  \n",
    "  ## 1. La Math√©matique : Mesurer la distance\n",
    "\n",
    "  Dans une base vectorielle, chaque donn√©e (mot, phrase, image) est un point dans un espace multidimensionnel (souvent 768, 1536 ou plus).  \n",
    "  Pour savoir si deux points sont \"similaires\", on mesure la **distance** ou l‚Äô**angle** entre eux.  \n",
    "\n",
    "  ### üîë Les 3 m√©triques principales\n",
    "  **A. Similarit√© Cosinus (Cosine Similarity)**  \n",
    "  - Mesure l‚Äôangle entre deux vecteurs.  \n",
    "  - 0¬∞ ‚Üí similarit√© = 1 (max).  \n",
    "  - 180¬∞ ‚Üí similarit√© = -1.  \n",
    "  - 90¬∞ ‚Üí aucune relation.  \n",
    "  - Usage : NLP et recherche de texte.  \n",
    "\n",
    "  **B. Distance Euclidienne (L2)**  \n",
    "  - Distance \"√† vol d‚Äôoiseau\" entre deux points.  \n",
    "  - Plus la valeur est petite, plus les vecteurs sont proches.  \n",
    "  - Usage : reconnaissance d‚Äôimages, magnitude importante.  \n",
    "\n",
    "  **C. Produit Scalaire (Dot Product)**  \n",
    "  - Multiplication des coordonn√©es des vecteurs.  \n",
    "  - Usage : syst√®mes de recommandation, apprentissage hybride.  \n",
    "\n",
    "  </div>\n",
    "\n",
    "  <!-- Colonne 2 : Algorithmique -->\n",
    "  <div style=\"flex: 1; padding: 15px; border: 2px solid #e65100; border-radius: 10px; background-color: #fff0e0; box-shadow: 2px 2px 8px rgba(0,0,0,0.1); color:#212121;\">\n",
    "  \n",
    "  ## 2. L'Algorithmique : Trouver vite (ANN)\n",
    "\n",
    "  La m√©thode na√Øve (KNN) compare chaque document un par un ‚Üí trop lent (O(N)).  \n",
    "  Les bases vectorielles utilisent des **algorithmes ANN (Approximate Nearest Neighbors)** pour gagner en vitesse.  \n",
    "\n",
    "  ### üöÄ Les 2 algorithmes stars\n",
    "  **A. HNSW (Hierarchical Navigable Small World)**  \n",
    "  - Graphe multi-couches : grands sauts (avion), moyens (train), petits (marche).  \n",
    "  - Avantages : rapide et pr√©cis.  \n",
    "  - Inconv√©nient : consomme beaucoup de RAM.  \n",
    "\n",
    "  **B. IVF (Inverted File Index ‚Äì FAISS)**  \n",
    "  - Division de l‚Äôespace en r√©gions (cellules de Voronoi).  \n",
    "  - Recherche uniquement dans la r√©gion la plus proche.  \n",
    "  - Avantages : r√©duit l‚Äôespace de recherche.  \n",
    "  - Inconv√©nient : n√©cessite un entra√Ænement initial (clustering).  \n",
    "\n",
    "  ### üìä En r√©sum√©\n",
    "  | Concept   | R√¥le                          | Exemple                        |\n",
    "  |-----------|-------------------------------|--------------------------------|\n",
    "  | Embedding | Transformer la donn√©e en chiffres | [0.1, -0.5, 0.8...]            |\n",
    "  | M√©trique  | Mesurer la distance           | Cosine vs Euclidean            |\n",
    "  | ANN Algo  | Chercher vite                 | HNSW (graphe multi-niveaux)    |\n",
    "\n",
    "  </div>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9ad602",
   "metadata": {},
   "source": [
    "## ORIENTE CLOUD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799e9341",
   "metadata": {},
   "source": [
    "Pinecone (SaaS, cloud g√©r√©) : https://www.pinecone.io/\n",
    "\n",
    "Qdrant (Open Source et Cloud) : https://qdrant.tech/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e124aef7",
   "metadata": {},
   "source": [
    "<img src=\"vectora ligne.jpg\" alt=\"Logo Python\" width=\"1500\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec744323",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; gap: 20px;\">\n",
    "\n",
    "  <!-- Colonne 1 : Pinecone -->\n",
    "  <div style=\"flex: 1; padding: 15px; border: 2px solid #e65100; border-radius: 10px; background-color: #fff0e0; box-shadow: 2px 2px 8px rgba(0,0,0,0.1); color:#212121;\">\n",
    "  \n",
    "  ## 3. Pinecone : Le service Cloud g√©r√© (SaaS)\n",
    "\n",
    "  **Nature**  \n",
    "  Base de donn√©es vectorielle enti√®rement g√©r√©e dans le cloud.  \n",
    "  üëâ Vous n‚Äôinstallez rien sur vos serveurs.  \n",
    "\n",
    "  **Forces**  \n",
    "  - üõ†Ô∏è **Z√©ro maintenance** : infrastructure, scaling, s√©curit√© et sauvegardes sont pris en charge.  \n",
    "  - üîí **Fiabilit√©** : haute disponibilit√© et performances garanties (SLA).  \n",
    "  - üéØ Choix id√©al pour les entreprises qui ne veulent pas g√©rer d‚Äôinfrastructure.  \n",
    "\n",
    "  **Faiblesses / Cas d‚Äôusage**  \n",
    "  - üí∞ **Co√ªt √©lev√©** si volume de donn√©es ou trafic important.  \n",
    "  - üåê **Donn√©es externes** : vos donn√©es quittent votre infrastructure pour aller sur les serveurs Pinecone.  \n",
    "  - üîí Logiciel propri√©taire (ferm√©).  \n",
    "\n",
    "  </div>\n",
    "\n",
    "  <!-- Colonne 2 : Qdrant -->\n",
    "  <div style=\"flex: 1; padding: 15px; border: 2px solid #e65100; border-radius: 10px; background-color: #fff0e0; box-shadow: 2px 2px 8px rgba(0,0,0,0.1); color:#212121;\">\n",
    "  \n",
    "  ## 3. Qdrant : Le serveur robuste et flexible (Open Source)\n",
    "\n",
    "  **Nature**  \n",
    "  Serveur de base de donn√©es vectorielle Open Source, √©crit en **Rust** (langage rapide et s√©curis√©).  \n",
    "\n",
    "  **Forces**  \n",
    "  - üîç **Recherche Hybride (Filtrage)** : combine recherche vectorielle et filtres traditionnels.  \n",
    "    Exemple : *¬´ Trouve-moi des documents similaires, MAIS seulement s‚Äôils datent de 2024 et sont de cat√©gorie Finance ¬ª*.  \n",
    "  - ‚ö° **Performance et stabilit√©** gr√¢ce √† Rust.  \n",
    "  - üê≥ H√©bergement possible via **Docker** (gratuit) ou version cloud g√©r√©e.  \n",
    "\n",
    "  **Faiblesses / Cas d‚Äôusage**  \n",
    "  - ‚úÖ Excellent choix pour garder le contr√¥le des donn√©es (auto-h√©bergement).  \n",
    "  - üîß Demande plus de configuration initiale que ChromaDB.  \n",
    "\n",
    "  </div>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451a3fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Installer les d√©pendances si besoin :\n",
    "# pip install pinecone-client langchain sentence-transformers\n",
    "\n",
    "import os\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import pinecone\n",
    "\n",
    "# 1. Initialiser Pinecone\n",
    "# Mets ta cl√© API et ton environnement (trouv√©s dans ton compte Pinecone)\n",
    "pinecone_api_key = \"YOUR_PINECONE_API_KEY\"\n",
    "pinecone_environment = \"us-east1-gcp\"  # ex: \"us-east1-gcp\"\n",
    "\n",
    "pinecone.init(api_key=pinecone_api_key, environment=pinecone_environment)\n",
    "\n",
    "# 2. Cr√©er les embeddings avec Hugging Face (mod√®le gratuit)\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# 3. Cr√©er l‚Äôindex Pinecone (si non existant)\n",
    "index_name = \"pdf-index\"\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(name=index_name, dimension=384)  \n",
    "    # 384 = dimension du mod√®le MiniLM-L6-v2\n",
    "\n",
    "# 4. Construire le vectorstore Pinecone √† partir de tes chunks\n",
    "# Supposons que tu as d√©j√† tes chunks (liste de Document LangChain)\n",
    "# chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "vectorstore = Pinecone.from_documents(\n",
    "    documents=pdf_chunks,\n",
    "    embedding=embeddings,\n",
    "    index_name=index_name\n",
    ")\n",
    "\n",
    "# 5. Faire une requ√™te de similarit√©\n",
    "query = \"Explique-moi la diff√©rence entre FAISS et ChromaDB\"\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "for i, res in enumerate(results):\n",
    "    print(f\"--- R√©sultat {i+1} ---\")\n",
    "    print(res.page_content[:300], \"...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07abf9c1",
   "metadata": {},
   "source": [
    "## Ressource complementaire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d9ea0d",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=6rATNqmpvmQ\n",
    "\n",
    "https://www.youtube.com/watch?v=jPYOrJHDXAQ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
