{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d4aa8a",
   "metadata": {},
   "source": [
    "## BASE DE DONNEES VECTORIELLE ET EMBEDING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc29c594",
   "metadata": {},
   "source": [
    "## ETAPE 1 CHUNKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1c8db05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def load_and_split_pdf_simple(pdf_path: str):\n",
    "    \"\"\"\n",
    "    Charge tout le PDF en un seul Document, puis le dÃ©coupe en chunks\n",
    "    avec chunk_size=1000 et chunk_overlap=100 (en caractÃ¨res).\n",
    "    \"\"\"\n",
    "    # 1. Extraire tout le texte du PDF\n",
    "    reader = PdfReader(pdf_path)\n",
    "    full_text = \"\"\n",
    "    for i, page in enumerate(reader.pages):\n",
    "        text = page.extract_text() or \"\"\n",
    "        full_text += text + f\"\\n\\n[PAGE {i+1}]\\n\\n\"  # Ajout de marqueur de page\n",
    "\n",
    "    # 2. CrÃ©er un seul Document\n",
    "    doc = Document(page_content=full_text.strip().replace('. .',''), metadata={\"source\": pdf_path})\n",
    "\n",
    "    # 3. DÃ©couper avec RecursiveCharacterTextSplitter (par dÃ©faut : len = caractÃ¨res)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "        separators=['.'],\n",
    "        keep_separator=True  # Garde les sÃ©parateurs pour Ã©viter de couper au milieu d'une phrase\n",
    "    )\n",
    "\n",
    "    # 4. Splitter\n",
    "    chunks = text_splitter.split_documents([doc])\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Utilisation\n",
    "pdf_chunks = load_and_split_pdf_simple(\"embedding_course_by_koulou.pdf\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d58bb5c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Lâ€™INTELLIGENCE ARTIFICIELLE\\nDes fondements thÃ©oriques aux bases de donnÃ©es vectorielles\\nAuteur : KOULOU crepin\\nDÃ©partement de Data Science\\n8 janvier 2026\\n\\n[PAGE 1]\\n\\nRÃ©sumÃ©\\nCe rapport prÃ©sente une analyse approfondie de lâ€™Ã©tat actuel de lâ€™Intelligence Artiï¬cielle.\\nNous explorerons dâ€™abord les fondements mathÃ©matiques de lâ€™apprentissage automatique\\n(Machine Learning) et des rÃ©seaux de neurones profonds (Deep Learning). Une attention\\nparticuliÃ¨re sera portÃ©e aux dÃ©veloppements rÃ©cents concernant les Grands ModÃ¨les de\\nLangage (LLM) et lâ€™infrastructure critique qui les soutient, notamment les bases de don-\\nnÃ©es vectorielles (FAISS, ChromaDB, Qdrant) et les mÃ©canismes de similaritÃ©. Enï¬n, nous\\naborderons les dÃ©ï¬s Ã©thiques et les perspectives dâ€™avenir.\\n\\n[PAGE 2]\\n\\nTable des matiÃ¨res\\n1 Introduction 2\\n1.1 Contexte Historique                . 2\\n1.2 DÃ©ï¬nitions et Typologie               2\\n2 Fondements MathÃ©matiques du Machine Learning 4\\n2.1 Lâ€™Apprentissage SupervisÃ©              . 4\\n2', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='.1 Lâ€™Apprentissage SupervisÃ©              . 4\\n2.2 RÃ©seaux de Neurones et RÃ©tropropagation          4\\n3 Deep Learning et Architecture Moderne 6\\n3.1 Les RÃ©seaux de Neurones Convolutifs (CNN)         . 6\\n3.2 Les Transformers et le MÃ©canisme dâ€™Attention         6\\n4 Infrastructure de lâ€™IA : Embeddings et Bases Vectorielles 8\\n4.1 Le Concept dâ€™Embedding (Plongement Lexical)        . 8\\n4.2 Mesure de la SimilaritÃ©               . 8\\n4.3 Les Bases de DonnÃ©es Vectorielles            . 8\\n4.3.1 Comparatif Technique             . 9\\n4.3.2 Exemple dâ€™implÃ©mentation (Python)         . 9\\n5 DÃ©ï¬s Ã‰thiques et Avenir 11\\n5.1 Biais Algorithmiques et Ã‰quitÃ©             . 11\\n5.2 RÃ©gulation (AI Act)                11\\n6 Conclusion 12\\n\\n[PAGE 3]\\n\\nChapitre 1\\nIntroduction\\nLâ€™Intelligence Artiï¬cielle (IA) a cessÃ© dâ€™Ãªtre un concept de science-ï¬ction pour devenir\\nune force motrice de lâ€™Ã©conomie mondiale.\\n1.1 Contexte Historique\\nDepuis le test de Turing jusquâ€™Ã  lâ€™avÃ¨nement de ChatGPT, lâ€™Ã©volution a Ã©tÃ© exponen-\\ntielle', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Les premiers systÃ¨mes experts ont laissÃ© place aux rÃ©seaux de neurones. Lâ€™intelli-\\ngence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©ment notre\\nsociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique permettent aux ordinateurs dâ€™ap-\\nprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque tÃ¢che. Dans\\nce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vectorielles jouent\\nun rÃ´le crucial pour traiter des informations non structurÃ©es. Ce paragraphe sert de rem-\\nplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©cier la mise\\nen page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques rÃ©centes, notamment\\nles architectures Transformers, ont ouvert la voie aux modÃ¨les de langage massifs (LLM).\\nLâ€™intelligence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©-\\nment notre sociÃ©tÃ©', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Les algorithmes dâ€™apprentissage automatique permettent aux ordina-\\nteurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque\\ntÃ¢che. Dans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vecto-\\nrielles jouent un rÃ´le crucial pour traiter des informations non structurÃ©es. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©-\\ncier la mise en page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques rÃ©centes,\\nnotamment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage\\nmassifs (LLM).\\n1.2 DÃ©ï¬nitions et Typologie\\nIl convient de distinguer lâ€™IA faible (ANI), spÃ©cialisÃ©e dans une tÃ¢che, de lâ€™IA forte\\n(AGI), hypothÃ©tique et capable de gÃ©nÃ©ralisation humaine. Lâ€™intelligence artiï¬cielle est un\\ndomaine en pleine expansion qui transforme profondÃ©ment notre sociÃ©tÃ©', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Les algorithmes\\ndâ€™apprentissage automatique permettent aux ordinateurs dâ€™apprendre Ã  partir de donnÃ©es\\nsans Ãªtre explicitement programmÃ©s pour chaque tÃ¢che. Dans ce contexte, les rÃ©seaux de\\nneuronesprofondsetlesbasesdedonnÃ©esvectoriellesjouentunrÃ´lecrucialpourtraiterdes\\n2\\n\\n[PAGE 4]\\n\\nÃ‰tat de lâ€™art Rapport IA 2024\\ninformations non structurÃ©es. Ce paragraphe sert de remplissage pour simuler le volume\\nde texte du rapport ï¬nal, permettant dâ€™apprÃ©cier la mise en page sans Ãªtre distrait par\\ndu latin. Les avancÃ©es technologiques rÃ©centes, notamment les architectures Transformers,\\nont ouvert la voie aux modÃ¨les de langage massifs (LLM).\\n3\\n\\n[PAGE 5]\\n\\nChapitre 2\\nFondements MathÃ©matiques du\\nMachine Learning\\nLâ€™IA repose avant tout sur lâ€™optimisation mathÃ©matique et les statistiques avancÃ©es.\\n2.1 Lâ€™Apprentissage SupervisÃ©\\nDans un cadre de rÃ©gression ou de classiï¬cation, nous cherchons Ã  minimiser une\\nfonction de coÃ»t (Loss Function).\\nSoit un jeu de donnÃ©esD = {(xi,yi)}n\\ni=1', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='.\\nSoit un jeu de donnÃ©esD = {(xi,yi)}n\\ni=1. Pour un modÃ¨le linÃ©airef(x) = wT x+ b,\\nnous cherchons Ã  minimiser lâ€™erreur quadratique moyenne (MSE) :\\nJ(w,b) = 1\\nn\\nnâˆ‘\\ni=1\\n(yi âˆ’(wT xi + b))2 (2.1)\\nLâ€™algorithme de descente de gradient permet de mettre Ã  jour les poidsw de maniÃ¨re\\nitÃ©rative :\\nwâ†wâˆ’Î±âˆ‡wJ(w,b) (2.2)\\noÃ¹ Î± reprÃ©sente le taux dâ€™apprentissage (learning rate), un hyperparamÃ¨tre crucial.\\n2.2 RÃ©seaux de Neurones et RÃ©tropropagation\\nUn neurone artiï¬ciel applique une fonction dâ€™activation non linÃ©aireÏƒ (comme ReLU\\nou Sigmoid) :\\na[l] = Ïƒ(W[l]a[lâˆ’1] + b[l]) (2.3)\\nLâ€™intelligence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©-\\nment notre sociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique permettent aux ordina-\\nteurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque\\ntÃ¢che. Dans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vecto-\\nrielles jouent un rÃ´le crucial pour traiter des informations non structurÃ©es', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©-\\ncier la mise en page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques rÃ©centes,\\n4\\n\\n[PAGE 6]\\n\\nÃ‰tat de lâ€™art Rapport IA 2024\\nnotamment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage\\nmassifs (LLM).\\nLâ€™intelligence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©-\\nment notre sociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique permettent aux ordina-\\nteurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque\\ntÃ¢che. Dans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vecto-\\nrielles jouent un rÃ´le crucial pour traiter des informations non structurÃ©es. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©-\\ncier la mise en page sans Ãªtre distrait par du latin', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Les avancÃ©es technologiques rÃ©centes,\\nnotamment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage\\nmassifs (LLM).\\nLâ€™intelligence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©-\\nment notre sociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique permettent aux ordina-\\nteurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque\\ntÃ¢che. Dans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vecto-\\nrielles jouent un rÃ´le crucial pour traiter des informations non structurÃ©es. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©-\\ncier la mise en page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques rÃ©centes,\\nnotamment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage\\nmassifs (LLM).\\nLâ€™intelligence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©-\\nment notre sociÃ©tÃ©', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Les algorithmes dâ€™apprentissage automatique permettent aux ordina-\\nteurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque\\ntÃ¢che. Dans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vecto-\\nrielles jouent un rÃ´le crucial pour traiter des informations non structurÃ©es. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©-\\ncier la mise en page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques rÃ©centes,\\nnotamment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage\\nmassifs (LLM).\\n5\\n\\n[PAGE 7]\\n\\nChapitre 3\\nDeep Learning et Architecture Moderne\\n3.1 Les RÃ©seaux de Neurones Convolutifs (CNN)\\nUtilisÃ©s principalement en vision par ordinateur pour la reconnaissance dâ€™images. Lâ€™in-\\ntelligence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©ment\\nnotre sociÃ©tÃ©', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Les algorithmes dâ€™apprentissage automatique permettent aux ordinateurs\\ndâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque tÃ¢che.\\nDans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vectorielles\\njouent un rÃ´le crucial pour traiter des informations non structurÃ©es. Ce paragraphe sert\\nde remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©cier la\\nmise en page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques rÃ©centes, notam-\\nment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage massifs\\n(LLM).\\n3.2 Les Transformers et le MÃ©canisme dâ€™Attention\\nLâ€™innovation majeure de lâ€™article fondateur de 2017 a rÃ©volutionnÃ© le traitement du\\nlangage naturel (NLP). Lâ€™attention se calcule ainsi :\\nAttention(Q,K,V ) =softmax\\n(QKT\\nâˆšdk\\n)\\nV (3.1)\\nCette architecture permet de parallÃ©liser les calculs, contrairement aux rÃ©seaux rÃ©cur-\\nrents (RNN) prÃ©cÃ©dents', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Lâ€™intelligence artiï¬cielle est un domaine en pleine expansion\\nqui transforme profondÃ©ment notre sociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique\\npermettent aux ordinateurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement pro-\\ngrammÃ©s pour chaque tÃ¢che. Dans ce contexte, les rÃ©seaux de neurones profonds et les\\nbases de donnÃ©es vectorielles jouent un rÃ´le crucial pour traiter des informations non\\nstructurÃ©es. Ce paragraphe sert de remplissage pour simuler le volume de texte du rap-\\nport ï¬nal, permettant dâ€™apprÃ©cier la mise en page sans Ãªtre distrait par du latin. Les\\navancÃ©es technologiques rÃ©centes, notamment les architectures Transformers, ont ouvert\\nla voie aux modÃ¨les de langage massifs (LLM).\\nLâ€™intelligence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©-\\nment notre sociÃ©tÃ©', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Les algorithmes dâ€™apprentissage automatique permettent aux ordina-\\nteurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque\\n6\\n\\n[PAGE 8]\\n\\nÃ‰tat de lâ€™art Rapport IA 2024\\ntÃ¢che. Dans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vecto-\\nrielles jouent un rÃ´le crucial pour traiter des informations non structurÃ©es. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©-\\ncier la mise en page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques rÃ©centes,\\nnotamment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage\\nmassifs (LLM).\\nLâ€™intelligence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©-\\nment notre sociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique permettent aux ordina-\\nteurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque\\ntÃ¢che', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Dans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vecto-\\nrielles jouent un rÃ´le crucial pour traiter des informations non structurÃ©es. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©-\\ncier la mise en page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques rÃ©centes,\\nnotamment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage\\nmassifs (LLM).\\nLâ€™intelligence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©-\\nment notre sociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique permettent aux ordina-\\nteurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque\\ntÃ¢che. Dans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vecto-\\nrielles jouent un rÃ´le crucial pour traiter des informations non structurÃ©es', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©-\\ncier la mise en page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques rÃ©centes,\\nnotamment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage\\nmassifs (LLM).\\n7\\n\\n[PAGE 9]\\n\\nChapitre 4\\nInfrastructure de lâ€™IA : Embeddings et\\nBases Vectorielles\\nCâ€™est ici que rÃ©side le cÅ“ur des systÃ¨mes RAG (GÃ©nÃ©ration AugmentÃ©e par la RÃ©cu-\\npÃ©ration) modernes.\\n4.1 Le Concept dâ€™Embedding (Plongement Lexical)\\nLes embeddings transforment des donnÃ©es non structurÃ©es (texte, image, son) en vec-\\nteurs denses de dimensiond. Lâ€™intelligence artiï¬cielle est un domaine en pleine expansion\\nqui transforme profondÃ©ment notre sociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique\\npermettent aux ordinateurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement pro-\\ngrammÃ©s pour chaque tÃ¢che', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Dans ce contexte, les rÃ©seaux de neurones profonds et les\\nbases de donnÃ©es vectorielles jouent un rÃ´le crucial pour traiter des informations non\\nstructurÃ©es. Ce paragraphe sert de remplissage pour simuler le volume de texte du rap-\\nport ï¬nal, permettant dâ€™apprÃ©cier la mise en page sans Ãªtre distrait par du latin. Les\\navancÃ©es technologiques rÃ©centes, notamment les architectures Transformers, ont ouvert\\nla voie aux modÃ¨les de langage massifs (LLM).\\n4.2 Mesure de la SimilaritÃ©\\nComme vu prÃ©cÃ©demment, la similaritÃ© cosinus est la mÃ©trique standard pour Ã©valuer\\nla proximitÃ© sÃ©mantique :\\nsim(A,B) = cos(Î¸) = AÂ·B\\nâˆ¥Aâˆ¥âˆ¥Bâˆ¥=\\nâˆ‘n\\ni=1 AiBiâˆšâˆ‘n\\ni=1 A2\\ni\\nâˆšâˆ‘n\\ni=1 B2\\ni\\n(4.1)\\n4.3 Les Bases de DonnÃ©es Vectorielles\\nPour gÃ©rer des millions de vecteurs, des solutions spÃ©cialisÃ©es sont nÃ©cessaires. Les\\nbases de donnÃ©es relationnelles classiques (SQL) ne sont pas optimisÃ©es pour ce type de\\ncalcul.\\n8\\n\\n[PAGE 10]\\n\\nÃ‰tat de lâ€™art Rapport IA 2024\\n4.3', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='.\\n8\\n\\n[PAGE 10]\\n\\nÃ‰tat de lâ€™art Rapport IA 2024\\n4.3.1 Comparatif Technique\\nâ€” FAISS (Meta): BibliothÃ¨que bas niveau, optimisÃ©e pour la recherche par simila-\\nritÃ© dense et le clustering. Utilise lâ€™indexation IVF.\\nâ€” ChromaDB : Solution open-source conviviale pour les dÃ©veloppeurs, idÃ©ale pour\\nles applications LLM lÃ©gÃ¨res et le prototypage.\\nâ€” Pinecone / Qdrant: Solutions gÃ©rÃ©es (SaaS) ou robustes (Rust) pour la produc-\\ntion Ã  grande Ã©chelle.\\n4.3.2 Exemple dâ€™implÃ©mentation (Python)\\nVoici comment on initialise une recherche simple avec la librairie ChromaDB :\\n1 import chromadb\\n2\\n3 # Initialisation du client (en mÃ© moire pour lâ€™ exemple )\\n4 client = chromadb . Client ()\\n5 collection = client . create_collection (\" mon_dataset_ia \")\\n6\\n7 # Ajout de documents ( texte brut + mÃ© tadonn Ã©es)\\n8 collection', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. add (\\n9 documents =[\"Lâ€™IA vectorielle est puissante \", \"Le chat mange des\\ncroquettes \"],\\n10 metadatas =[{ \" source \": \" cours \"}, {\" source \": \" vie \"}] ,\\n11 ids =[\" id1 \", \" id2 \"]\\n12 )\\n13\\n14 # Recherche sÃ© mantique (Le plus proche voisin )\\n15 results = collection . query (\\n16 query_texts =[\" Base de donn Ã©es vecteur \"],\\n17 n_results =1\\n18 )\\n19\\nListing 4.1 â€“ Exemple de code ChromaDB pour la recherche sÃ©mantique\\nLâ€™intelligence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©-\\nment notre sociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique permettent aux ordina-\\nteurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque\\ntÃ¢che. Dans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vecto-\\nrielles jouent un rÃ´le crucial pour traiter des informations non structurÃ©es. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©-\\ncier la mise en page sans Ãªtre distrait par du latin', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Les avancÃ©es technologiques rÃ©centes,\\nnotamment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage\\nmassifs (LLM).\\nLâ€™intelligence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©-\\nment notre sociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique permettent aux ordina-\\nteurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque\\ntÃ¢che. Dans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vecto-\\nrielles jouent un rÃ´le crucial pour traiter des informations non structurÃ©es. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©-\\ncier la mise en page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques rÃ©centes,\\nnotamment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage\\nmassifs (LLM)', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='.\\n9\\n\\n[PAGE 11]\\n\\nÃ‰tat de lâ€™art Rapport IA 2024\\nLâ€™intelligence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©-\\nment notre sociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique permettent aux ordina-\\nteurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque\\ntÃ¢che. Dans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vecto-\\nrielles jouent un rÃ´le crucial pour traiter des informations non structurÃ©es. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©-\\ncier la mise en page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques rÃ©centes,\\nnotamment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage\\nmassifs (LLM).\\nLâ€™intelligence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©-\\nment notre sociÃ©tÃ©', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Les algorithmes dâ€™apprentissage automatique permettent aux ordina-\\nteurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque\\ntÃ¢che. Dans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vecto-\\nrielles jouent un rÃ´le crucial pour traiter des informations non structurÃ©es. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©-\\ncier la mise en page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques rÃ©centes,\\nnotamment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage\\nmassifs (LLM).\\n10\\n\\n[PAGE 12]\\n\\nChapitre 5\\nDÃ©ï¬s Ã‰thiques et Avenir\\n5.1 Biais Algorithmiques et Ã‰quitÃ©\\nLes modÃ¨les dâ€™IA peuvent reproduire, voire ampliï¬er, les biais prÃ©sents dans leurs\\ndonnÃ©es dâ€™entraÃ®nement. Lâ€™intelligence artiï¬cielle est un domaine en pleine expansion\\nqui transforme profondÃ©ment notre sociÃ©tÃ©', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Les algorithmes dâ€™apprentissage automatique\\npermettent aux ordinateurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement pro-\\ngrammÃ©s pour chaque tÃ¢che. Dans ce contexte, les rÃ©seaux de neurones profonds et les\\nbases de donnÃ©es vectorielles jouent un rÃ´le crucial pour traiter des informations non\\nstructurÃ©es. Ce paragraphe sert de remplissage pour simuler le volume de texte du rap-\\nport ï¬nal, permettant dâ€™apprÃ©cier la mise en page sans Ãªtre distrait par du latin. Les\\navancÃ©es technologiques rÃ©centes, notamment les architectures Transformers, ont ouvert\\nla voie aux modÃ¨les de langage massifs (LLM).\\n5.2 RÃ©gulation (AI Act)\\nLâ€™Union EuropÃ©enne et dâ€™autres instances travaillent sur des cadres lÃ©gaux. Lâ€™intelli-\\ngence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©ment notre\\nsociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique permettent aux ordinateurs dâ€™ap-\\nprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque tÃ¢che', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Dans\\nce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vectorielles jouent\\nun rÃ´le crucial pour traiter des informations non structurÃ©es. Ce paragraphe sert de rem-\\nplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©cier la mise\\nen page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques rÃ©centes, notamment\\nles architectures Transformers, ont ouvert la voie aux modÃ¨les de langage massifs (LLM).\\nLâ€™intelligence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©-\\nment notre sociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique permettent aux ordina-\\nteurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque\\ntÃ¢che. Dans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vecto-\\nrielles jouent un rÃ´le crucial pour traiter des informations non structurÃ©es', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©-\\ncier la mise en page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques rÃ©centes,\\nnotamment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage\\nmassifs (LLM).\\n11\\n\\n[PAGE 13]\\n\\nChapitre 6\\nConclusion\\nEn conclusion, lâ€™intelligence artiï¬cielle traverse une phase de maturation industrielle.\\nLâ€™intÃ©gration des bases de donnÃ©es vectorielles comme mÃ©moire Ã  long terme pour les\\nLLM ouvre la voie Ã  des agents autonomes plus performants et contextuels. Lâ€™intelli-\\ngence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©ment notre\\nsociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique permettent aux ordinateurs dâ€™ap-\\nprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque tÃ¢che', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       " Document(page_content='. Dans\\nce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vectorielles jouent\\nun rÃ´le crucial pour traiter des informations non structurÃ©es. Ce paragraphe sert de rem-\\nplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©cier la mise\\nen page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques rÃ©centes, notamment\\nles architectures Transformers, ont ouvert la voie aux modÃ¨les de langage massifs (LLM).\\n12\\n\\n[PAGE 14]\\n\\nBibliographie\\n[1] Vaswani, A., et al. (2017).Attention Is All You Need(Lâ€™attention est tout ce dont\\nvous avez besoin). Advances in Neural Information Processing Systems.\\n[2] LeCun, Y., Bengio, Y., & Hinton, G. (2015).Deep learning(Lâ€™apprentissage profond).\\nNature, 521(7553), 436-444.\\n[3] Johnson, J., Douze, M., & JÃ©gou, H. (2017).Billion-scale similarity search with GPUs\\n(Recherche de similaritÃ© Ã  lâ€™Ã©chelle du milliard avec GPU). IEEE Transactions on Big\\nData.\\n13\\n\\n[PAGE 15]', metadata={'source': 'embedding_course_by_koulou.pdf'})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e3c8d4",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"faiss chroma.jpg\" alt=\"Logo Python\" width=\"1800\" height=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e92ee7",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; gap: 20px;\">\n",
    "\n",
    "  <!-- Colonne 1 -->\n",
    "  <div style=\"flex: 1; padding: 15px; border: 2px solid #e65100; border-radius: 10px; background-color: #fff0e0; box-shadow: 2px 2px 8px rgba(0,0,0,0.1); color:#212121;\">\n",
    "  \n",
    "  ## 1. Introduction GÃ©nÃ©rale : Les Bases de DonnÃ©es Vectorielles\n",
    "\n",
    "  **Qu'est-ce que c'est ?**  \n",
    "  Une base de donnÃ©es vectorielle est un systÃ¨me optimisÃ© pour stocker et interroger des *vecteurs* (embeddings).\n",
    "\n",
    "  **Pourquoi en avons-nous besoin ?**  \n",
    "  Les bases traditionnelles (SQL, NoSQL) gÃ¨rent bien les correspondances exactes, mais Ã©chouent sur la *similaritÃ©*.  \n",
    "\n",
    "  Avec lâ€™essor de lâ€™IA, les donnÃ©es complexes (texte, images, audio) sont transformÃ©es en vecteurs.  \n",
    "  âœ¨ Des concepts proches se retrouvent mathÃ©matiquement voisins dans un espace multidimensionnel.  \n",
    "\n",
    "  ğŸ‘‰ Fonction principale : **Vector Search**.  \n",
    "  Exemple : un vecteur \"chat\" renvoie les images les plus proches dâ€™un chat, mÃªme sans mot-clÃ©.  \n",
    "\n",
    "  Câ€™est le cÅ“ur des applications modernes comme le **RAG (Retrieval-Augmented Generation)**, qui permet aux LLM dâ€™accÃ©der Ã  vos documents.  \n",
    "\n",
    "  </div>\n",
    "\n",
    "  <!-- Colonne 2 -->\n",
    "  <div style=\"flex: 1; padding: 15px; border: 2px solid #e65100; border-radius: 10px; background-color: #fff0e0; box-shadow: 2px 2px 8px rgba(0,0,0,0.1); color:#212121;\">\n",
    "  \n",
    "  ## 2. FAISS et ChromaDB : Solutions orientÃ©es \"DÃ©veloppement Local\"\n",
    "\n",
    "  **FAISS (Facebook AI Similarity Search)**  \n",
    "  - Origine : Meta (Facebook AI Research).  \n",
    "  - Nature : BibliothÃ¨que C++/Python, ultra performante.  \n",
    "  - Forces :  \n",
    "    - ğŸš€ Recherche sur milliards de vecteurs.  \n",
    "    - âš¡ Optimisation GPU.  \n",
    "  - Faiblesses :  \n",
    "    - Bas niveau, pas de stockage persistant.  \n",
    "    - Risque de perte si non sauvegardÃ©.  \n",
    "\n",
    "  **ChromaDB**  \n",
    "  - Origine : Projet Open Source rÃ©cent.  \n",
    "  - Nature : Base vectorielle simple et rapide.  \n",
    "  - Forces :  \n",
    "    - ğŸ¯ Installation facile (`pip install chromadb`).  \n",
    "    - ğŸ”„ Vectorisation intÃ©grÃ©e.  \n",
    "  - Faiblesses :  \n",
    "    - IdÃ©al pour petits/moyens projets.  \n",
    "    - Moins robuste pour tÃ©raoctets en production.  \n",
    "\n",
    "  </div>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e1ff90",
   "metadata": {},
   "source": [
    "## FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc849efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def build_faiss_index(chunks):\n",
    "    \"\"\"\n",
    "    Construit un index FAISS Ã  partir de chunks dÃ©jÃ  prÃ©parÃ©s.\n",
    "    Utilise un modÃ¨le gratuit Hugging Face pour gÃ©nÃ©rer les embeddings.\n",
    "    \"\"\"\n",
    "    # 1. CrÃ©er les embeddings avec Hugging Face\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"  # modÃ¨le open-source ou openaiembeddings\n",
    "    )\n",
    "\n",
    "    # 2. Construire lâ€™index FAISS\n",
    "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "    return vectorstore\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a413f039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RÃ©sultat 1 ---\n",
      ".\n",
      "8\n",
      "\n",
      "[PAGE 10]\n",
      "\n",
      "Ã‰tat de lâ€™art Rapport IA 2024\n",
      "4.3.1 Comparatif Technique\n",
      "â€” FAISS (Meta): BibliothÃ¨que bas niveau, optimisÃ©e pour la recherche par simila-\n",
      "ritÃ© dense et le clustering. Utilise lâ€™indexation IVF.\n",
      "â€” ChromaDB : Solution open-source conviviale pour les dÃ©veloppeurs, idÃ©ale pour\n",
      "les applications LLM lÃ©gÃ¨res et le prototypage.\n",
      "â€” Pinecone / Qdrant: Solutions gÃ©rÃ©es (SaaS) ou robustes (Rust) pour la produc-\n",
      "tion Ã  grande Ã©chelle.\n",
      "4.3.2 Exemple dâ€™implÃ©mentation (Python)\n",
      "Voici comment on ini ...\n",
      "--- RÃ©sultat 2 ---\n",
      ". Dans ce contexte, les rÃ©seaux de neurones profonds et les\n",
      "bases de donnÃ©es vectorielles jouent un rÃ´le crucial pour traiter des informations non\n",
      "structurÃ©es. Ce paragraphe sert de remplissage pour simuler le volume de texte du rap-\n",
      "port ï¬nal, permettant dâ€™apprÃ©cier la mise en page sans Ãªtre distrait par du latin. Les\n",
      "avancÃ©es technologiques rÃ©centes, notamment les architectures Transformers, ont ouvert\n",
      "la voie aux modÃ¨les de langage massifs (LLM).\n",
      "4.2 Mesure de la SimilaritÃ©\n",
      "Comme vu prÃ©cÃ©demm ...\n",
      "--- RÃ©sultat 3 ---\n",
      ". add (\n",
      "9 documents =[\"Lâ€™IA vectorielle est puissante \", \"Le chat mange des\n",
      "croquettes \"],\n",
      "10 metadatas =[{ \" source \": \" cours \"}, {\" source \": \" vie \"}] ,\n",
      "11 ids =[\" id1 \", \" id2 \"]\n",
      "12 )\n",
      "13\n",
      "14 # Recherche sÃ© mantique (Le plus proche voisin )\n",
      "15 results = collection . query (\n",
      "16 query_texts =[\" Base de donn Ã©es vecteur \"],\n",
      "17 n_results =1\n",
      "18 )\n",
      "19\n",
      "Listing 4.1 â€“ Exemple de code ChromaDB pour la recherche sÃ©mantique\n",
      "Lâ€™intelligence artiï¬cielle est un domaine en pleine expansion qui transforme profo ...\n",
      "--- RÃ©sultat 4 ---\n",
      ". Ce paragraphe\n",
      "sert de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©-\n",
      "cier la mise en page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques rÃ©centes,\n",
      "notamment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage\n",
      "massifs (LLM).\n",
      "7\n",
      "\n",
      "[PAGE 9]\n",
      "\n",
      "Chapitre 4\n",
      "Infrastructure de lâ€™IA : Embeddings et\n",
      "Bases Vectorielles\n",
      "Câ€™est ici que rÃ©side le cÅ“ur des systÃ¨mes RAG (GÃ©nÃ©ration AugmentÃ©e par la RÃ©cu-\n",
      "pÃ©ration) modernes.\n",
      "4.1 Le Concept dâ€™Embe ...\n",
      "--- RÃ©sultat 5 ---\n",
      ". Les algorithmes dâ€™apprentissage automatique permettent aux ordinateurs\n",
      "dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque tÃ¢che.\n",
      "Dans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vectorielles\n",
      "jouent un rÃ´le crucial pour traiter des informations non structurÃ©es. Ce paragraphe sert\n",
      "de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©cier la\n",
      "mise en page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques  ...\n",
      "--- RÃ©sultat 6 ---\n",
      ". Les avancÃ©es technologiques rÃ©centes,\n",
      "notamment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage\n",
      "massifs (LLM).\n",
      "Lâ€™intelligence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©-\n",
      "ment notre sociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique permettent aux ordina-\n",
      "teurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque\n",
      "tÃ¢che. Dans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vecto-\n",
      "rielles jouen ...\n",
      "--- RÃ©sultat 7 ---\n",
      ". Les avancÃ©es technologiques rÃ©centes,\n",
      "notamment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage\n",
      "massifs (LLM).\n",
      "Lâ€™intelligence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©-\n",
      "ment notre sociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique permettent aux ordina-\n",
      "teurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque\n",
      "tÃ¢che. Dans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vecto-\n",
      "rielles jouen ...\n",
      "--- RÃ©sultat 8 ---\n",
      ". Lâ€™intelligence artiï¬cielle est un domaine en pleine expansion\n",
      "qui transforme profondÃ©ment notre sociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique\n",
      "permettent aux ordinateurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement pro-\n",
      "grammÃ©s pour chaque tÃ¢che. Dans ce contexte, les rÃ©seaux de neurones profonds et les\n",
      "bases de donnÃ©es vectorielles jouent un rÃ´le crucial pour traiter des informations non\n",
      "structurÃ©es. Ce paragraphe sert de remplissage pour simuler le volume de texte du rap-\n",
      "po ...\n",
      "--- RÃ©sultat 9 ---\n",
      ". Les premiers systÃ¨mes experts ont laissÃ© place aux rÃ©seaux de neurones. Lâ€™intelli-\n",
      "gence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©ment notre\n",
      "sociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique permettent aux ordinateurs dâ€™ap-\n",
      "prendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque tÃ¢che. Dans\n",
      "ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vectorielles jouent\n",
      "un rÃ´le crucial pour traiter des informations non structurÃ©es. Ce ...\n",
      "--- RÃ©sultat 10 ---\n",
      ". Les algorithmes dâ€™apprentissage automatique permettent aux ordina-\n",
      "teurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque\n",
      "tÃ¢che. Dans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vecto-\n",
      "rielles jouent un rÃ´le crucial pour traiter des informations non structurÃ©es. Ce paragraphe\n",
      "sert de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©-\n",
      "cier la mise en page sans Ãªtre distrait par du latin. Les avancÃ©es technolog ...\n"
     ]
    }
   ],
   "source": [
    "# Supposons que tu as dÃ©jÃ  tes chunks :\n",
    "# chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "faiss_index = build_faiss_index(pdf_chunks)\n",
    "\n",
    "# Faire une requÃªte\n",
    "query = \"Explique-moi la diffÃ©rence entre FAISS et ChromaDB\"\n",
    "results = faiss_index.similarity_search(query, k=10)\n",
    "\n",
    "# Afficher les rÃ©sultats\n",
    "for i, res in enumerate(results):\n",
    "    print(f\"--- RÃ©sultat {i+1} ---\")\n",
    "    print(res.page_content[:500], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f004caa",
   "metadata": {},
   "source": [
    "<img src=\"reranking.jpg\" alt=\"Logo Python\" width=\"1800\" height=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3df822",
   "metadata": {},
   "source": [
    "## Re-Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc90c0f0",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; gap: 20px;\">\n",
    "\n",
    "  <!-- Colonne 1 -->\n",
    "  <div style=\"flex: 1; padding: 15px; border: 2px solid #e65100; border-radius: 10px; background-color: #fff0e0; box-shadow: 2px 2px 8px rgba(0,0,0,0.1); color:#212121;\">\n",
    "  \n",
    "  ## 1. Reranking : Concept et Pourquoi lâ€™utiliser\n",
    "\n",
    "  **Qu'estâ€‘ce que le reranking ?**  \n",
    "  Le *reranking* est une Ã©tape secondaire qui rÃ©ordonne une liste de rÃ©sultats initialement rÃ©cupÃ©rÃ©s (par ex. via FAISS) en utilisant un modÃ¨le plus fin ou une mÃ©thode diffÃ©rente pour mieux estimer la pertinence.\n",
    "\n",
    "  **Pourquoi lâ€™ajouter ?**  \n",
    "  - Les recherches vectorielles (FAISS, ChromaDB) fournissent un bon *prÃ©croisement* basÃ© sur la proximitÃ© dans lâ€™espace des embeddings.  \n",
    "  - Le reranking applique une Ã©valuation sÃ©mantique plus riche (modÃ¨les de rerank, cross-encoders) pour corriger les faux positifs et amÃ©liorer la prÃ©cision en tÃªte de liste.\n",
    "\n",
    "  **Flux typique**  \n",
    "  1. **Retrieval** : rÃ©cupÃ©rer topâ€‘k via FAISS (rapide).  \n",
    "  2. **Rerank** : envoyer ces k documents + la requÃªte Ã  un modÃ¨le de reranking.  \n",
    "  3. **RÃ©ordonnancement** : trier par score de rerank et retourner topâ€‘n final.\n",
    "\n",
    "  **Avantages**  \n",
    "  - ğŸ” Meilleure prÃ©cision en tÃªte de rÃ©sultats.  \n",
    "  - âš–ï¸ Moins de bruit : on rÃ©duit les documents proches mais non pertinents.  \n",
    "  - ğŸ’¡ ComplÃ©mentaritÃ© : combine vitesse (FAISS) et finesse (reranker).\n",
    "\n",
    "  **Limites**  \n",
    "  - â±ï¸ CoÃ»t et latence supplÃ©mentaires (appel au modÃ¨le de rerank).  \n",
    "  - ğŸ’¸ CoÃ»t monÃ©taire si le reranker est un service payant.  \n",
    "  - ğŸ” NÃ©cessite un bon prÃ©traitement (nettoyage, dÃ©coupage) pour Ãªtre efficace.\n",
    "\n",
    "  **Quand lâ€™utiliser ?**  \n",
    "  - Quand la qualitÃ© des 3â€“5 premiers rÃ©sultats est critique (chatbot, RAG, FAQ).  \n",
    "  - Quand la base contient beaucoup de documents proches sÃ©mantiquement mais peu pertinents.\n",
    "\n",
    "  </div>\n",
    "\n",
    "  <!-- Colonne 2 -->\n",
    "  <div style=\"flex: 1; padding: 15px; border: 2px solid #e65100; border-radius: 10px; background-color: #fff0e0; box-shadow: 2px 2px 8px rgba(0,0,0,0.1); color:#212121;\">\n",
    "  \n",
    "  ## 2. Cohere et Reranking : Mise en Å“uvre pratique\n",
    "\n",
    "  **Pourquoi Cohere pour le rerank ?**  \n",
    "  Cohere propose des modÃ¨les dÃ©diÃ©s au *reranking* (crossâ€‘encoder style) qui comparent directement la requÃªte et chaque document pour produire un score de pertinence fin.\n",
    "\n",
    "  **Pattern dâ€™intÃ©gration**  \n",
    "  - **Ã‰tape 1** : FAISS â†’ `similarity_search(query, k=K)`  \n",
    "  - **Ã‰tape 2** : Extraire `page_content` des K rÃ©sultats.  \n",
    "  - **Ã‰tape 3** : Appel Cohere Rerank avec `query` + `documents`.  \n",
    "  - **Ã‰tape 4** : RÃ©ordonner les objets FAISS selon `relevance_score` renvoyÃ© par Cohere.\n",
    "\n",
    "  **Bonnes pratiques**  \n",
    "  - **Choisir le modÃ¨le** : `rerank-multilingual` si documents en franÃ§ais; `rerank-english` pour lâ€™anglais.  \n",
    "  - **Limiter K** : 10â€“50 selon latence/coÃ»t.  \n",
    "  - **Fallback** : prÃ©voir un retour aux rÃ©sultats FAISS si lâ€™API Ã©choue.  \n",
    "  - **Conserver mÃ©tadonnÃ©es** : renvoyer lâ€™objet FAISS original pour garder source, offset, page, etc.  \n",
    "  - **Batching** : grouper les appels si possible pour rÃ©duire latence et coÃ»t.\n",
    "\n",
    "  **Exemples dâ€™amÃ©liorations observables**  \n",
    "  - Augmentation de la prÃ©cision @1 et @3.  \n",
    "  - RÃ©duction des hallucinations dans les rÃ©ponses RAG.  \n",
    "  - Meilleure cohÃ©rence des extraits utilisÃ©s pour la gÃ©nÃ©ration.\n",
    "\n",
    "  **Risques et mitigations**  \n",
    "  - **Latence** â†’ mettre en cache les reranks frÃ©quents.  \n",
    "  - **CoÃ»t** â†’ nâ€™utiliser le rerank que pour les requÃªtes critiques.  \n",
    "  - **Biais** â†’ tester le reranker sur jeux de requÃªtes reprÃ©sentatifs.\n",
    "\n",
    "  **Astuce rapide**  \n",
    "  Combine un score FAISS (similitude) et le score Cohere (rerank) via une pondÃ©ration pour tirer parti des deux signaux avant le tri final.\n",
    "\n",
    "  </div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ad05b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Configuration globale ===\n",
    "# Remplace par ta clÃ© API Cohere\n",
    "COHERE_API_KEY = \"votre cle api cohere\"  # âš ï¸ Ã€ remplacer ou Ã  charger depuis .env / config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2e380e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "\n",
    "# Initialiser le client Cohere\n",
    "co = cohere.Client(COHERE_API_KEY)\n",
    "\n",
    "# === Fonction principale : Rerank avec Cohere ===\n",
    "def rerank_with_cohere(\n",
    "    query: str,\n",
    "    faiss_results: List[Any],\n",
    "    top_n: int = 5,\n",
    "    model: str = \"rerank-multilingual-v2.0\",\n",
    "    timeout: int = 30\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Rerank les rÃ©sultats FAISS avec l'API Cohere Rerank.\n",
    "    \n",
    "    Args:\n",
    "        query (str): RequÃªte utilisateur.\n",
    "        faiss_results (List[Any]): Liste d'objets retournÃ©s par `faiss_index.similarity_search`.\n",
    "        top_n (int): Nombre de rÃ©sultats Ã  retourner aprÃ¨s reranking.\n",
    "        model (str): ModÃ¨le Cohere Ã  utiliser (ex: \"rerank-english-v3.0\").\n",
    "        timeout (int): Timeout en secondes pour l'appel API.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: Liste d'Ã©lÃ©ments contenant :\n",
    "            - 'doc': objet FAISS original\n",
    "            - 'original_index': index dans la liste d'origine\n",
    "            - 'score': score de pertinence (float) ou None si erreur\n",
    "    \"\"\"\n",
    "    if not faiss_results:\n",
    "        return []\n",
    "\n",
    "    # Extraire les textes des documents + garder trace des indices\n",
    "    documents = []\n",
    "    for i, res in enumerate(faiss_results):\n",
    "        text = getattr(res, \"page_content\", None) or getattr(res, \"content\", None)\n",
    "        if not text:\n",
    "            text = str(res)  # fallback\n",
    "        documents.append(text)\n",
    "\n",
    "    # Appel Ã  l'API Cohere avec gestion d'erreur\n",
    "    try:\n",
    "        response = co.rerank(\n",
    "            model=model,\n",
    "            query=query,\n",
    "            documents=documents,\n",
    "            top_n=min(top_n, len(documents)),\n",
    "            timeout=timeout\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"[âš ï¸ Warning] Cohere rerank failed: {e}\")\n",
    "        # Retourne les premiers rÃ©sultats sans reranking\n",
    "        return [\n",
    "            {\"doc\": faiss_results[i], \"original_index\": i, \"score\": None}\n",
    "            for i in range(min(top_n, len(faiss_results)))\n",
    "        ]\n",
    "\n",
    "    # Extraire les rÃ©sultats (compatible avec diffÃ©rentes versions du SDK)\n",
    "    results_list = getattr(response, \"results\", None)\n",
    "    if results_list is None:\n",
    "        results_list = response  # fallback pour anciennes versions\n",
    "\n",
    "    # Construire la liste rerankÃ©e\n",
    "    reranked = []\n",
    "    for item in results_list:\n",
    "        idx = getattr(item, \"index\", None)\n",
    "        score = getattr(item, \"relevance_score\", None) or getattr(item, \"score\", None)\n",
    "\n",
    "        if idx is None or idx >= len(faiss_results):\n",
    "            continue  # Ignorer les Ã©lÃ©ments invalides\n",
    "\n",
    "        reranked.append({\n",
    "            \"doc\": faiss_results[idx],\n",
    "            \"original_index\": idx,\n",
    "            \"score\": float(score) if score is not None else None\n",
    "        })\n",
    "\n",
    "    return reranked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c915a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[âš ï¸ Warning] Cohere rerank failed: rerank() got an unexpected keyword argument 'timeout'\n",
      "5 rÃ©sultats aprÃ¨s reranking avec Cohere :\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Exemple d'utilisation ===\n",
    "# âš ï¸ Assure-toi que `faiss_index` est dÃ©fini avant cet appel\n",
    "# Exemple fictif :\n",
    "# from some_module import faiss_index\n",
    "\n",
    "query = \"Explique-moi la diffÃ©rence entre FAISS et ChromaDB\"\n",
    "faiss_results = faiss_index.similarity_search(query, k=10)\n",
    "\n",
    "reranked_results = rerank_with_cohere(query, faiss_results, top_n=5)\n",
    "\n",
    "print(len(reranked_results), \"rÃ©sultats aprÃ¨s reranking avec Cohere :\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75341be2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'doc': Document(page_content='.\\n8\\n\\n[PAGE 10]\\n\\nÃ‰tat de lâ€™art Rapport IA 2024\\n4.3.1 Comparatif Technique\\nâ€” FAISS (Meta): BibliothÃ¨que bas niveau, optimisÃ©e pour la recherche par simila-\\nritÃ© dense et le clustering. Utilise lâ€™indexation IVF.\\nâ€” ChromaDB : Solution open-source conviviale pour les dÃ©veloppeurs, idÃ©ale pour\\nles applications LLM lÃ©gÃ¨res et le prototypage.\\nâ€” Pinecone / Qdrant: Solutions gÃ©rÃ©es (SaaS) ou robustes (Rust) pour la produc-\\ntion Ã  grande Ã©chelle.\\n4.3.2 Exemple dâ€™implÃ©mentation (Python)\\nVoici comment on initialise une recherche simple avec la librairie ChromaDB :\\n1 import chromadb\\n2\\n3 # Initialisation du client (en mÃ© moire pour lâ€™ exemple )\\n4 client = chromadb . Client ()\\n5 collection = client . create_collection (\" mon_dataset_ia \")\\n6\\n7 # Ajout de documents ( texte brut + mÃ© tadonn Ã©es)\\n8 collection', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       "  'original_index': 0,\n",
       "  'score': None},\n",
       " {'doc': Document(page_content='. Dans ce contexte, les rÃ©seaux de neurones profonds et les\\nbases de donnÃ©es vectorielles jouent un rÃ´le crucial pour traiter des informations non\\nstructurÃ©es. Ce paragraphe sert de remplissage pour simuler le volume de texte du rap-\\nport ï¬nal, permettant dâ€™apprÃ©cier la mise en page sans Ãªtre distrait par du latin. Les\\navancÃ©es technologiques rÃ©centes, notamment les architectures Transformers, ont ouvert\\nla voie aux modÃ¨les de langage massifs (LLM).\\n4.2 Mesure de la SimilaritÃ©\\nComme vu prÃ©cÃ©demment, la similaritÃ© cosinus est la mÃ©trique standard pour Ã©valuer\\nla proximitÃ© sÃ©mantique :\\nsim(A,B) = cos(Î¸) = AÂ·B\\nâˆ¥Aâˆ¥âˆ¥Bâˆ¥=\\nâˆ‘n\\ni=1 AiBiâˆšâˆ‘n\\ni=1 A2\\ni\\nâˆšâˆ‘n\\ni=1 B2\\ni\\n(4.1)\\n4.3 Les Bases de DonnÃ©es Vectorielles\\nPour gÃ©rer des millions de vecteurs, des solutions spÃ©cialisÃ©es sont nÃ©cessaires. Les\\nbases de donnÃ©es relationnelles classiques (SQL) ne sont pas optimisÃ©es pour ce type de\\ncalcul.\\n8\\n\\n[PAGE 10]\\n\\nÃ‰tat de lâ€™art Rapport IA 2024\\n4.3', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       "  'original_index': 1,\n",
       "  'score': None},\n",
       " {'doc': Document(page_content='. add (\\n9 documents =[\"Lâ€™IA vectorielle est puissante \", \"Le chat mange des\\ncroquettes \"],\\n10 metadatas =[{ \" source \": \" cours \"}, {\" source \": \" vie \"}] ,\\n11 ids =[\" id1 \", \" id2 \"]\\n12 )\\n13\\n14 # Recherche sÃ© mantique (Le plus proche voisin )\\n15 results = collection . query (\\n16 query_texts =[\" Base de donn Ã©es vecteur \"],\\n17 n_results =1\\n18 )\\n19\\nListing 4.1 â€“ Exemple de code ChromaDB pour la recherche sÃ©mantique\\nLâ€™intelligence artiï¬cielle est un domaine en pleine expansion qui transforme profondÃ©-\\nment notre sociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique permettent aux ordina-\\nteurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque\\ntÃ¢che. Dans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vecto-\\nrielles jouent un rÃ´le crucial pour traiter des informations non structurÃ©es. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©-\\ncier la mise en page sans Ãªtre distrait par du latin', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       "  'original_index': 2,\n",
       "  'score': None},\n",
       " {'doc': Document(page_content='. Ce paragraphe\\nsert de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©-\\ncier la mise en page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques rÃ©centes,\\nnotamment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage\\nmassifs (LLM).\\n7\\n\\n[PAGE 9]\\n\\nChapitre 4\\nInfrastructure de lâ€™IA : Embeddings et\\nBases Vectorielles\\nCâ€™est ici que rÃ©side le cÅ“ur des systÃ¨mes RAG (GÃ©nÃ©ration AugmentÃ©e par la RÃ©cu-\\npÃ©ration) modernes.\\n4.1 Le Concept dâ€™Embedding (Plongement Lexical)\\nLes embeddings transforment des donnÃ©es non structurÃ©es (texte, image, son) en vec-\\nteurs denses de dimensiond. Lâ€™intelligence artiï¬cielle est un domaine en pleine expansion\\nqui transforme profondÃ©ment notre sociÃ©tÃ©. Les algorithmes dâ€™apprentissage automatique\\npermettent aux ordinateurs dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement pro-\\ngrammÃ©s pour chaque tÃ¢che', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       "  'original_index': 3,\n",
       "  'score': None},\n",
       " {'doc': Document(page_content='. Les algorithmes dâ€™apprentissage automatique permettent aux ordinateurs\\ndâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque tÃ¢che.\\nDans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vectorielles\\njouent un rÃ´le crucial pour traiter des informations non structurÃ©es. Ce paragraphe sert\\nde remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©cier la\\nmise en page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques rÃ©centes, notam-\\nment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage massifs\\n(LLM).\\n3.2 Les Transformers et le MÃ©canisme dâ€™Attention\\nLâ€™innovation majeure de lâ€™article fondateur de 2017 a rÃ©volutionnÃ© le traitement du\\nlangage naturel (NLP). Lâ€™attention se calcule ainsi :\\nAttention(Q,K,V ) =softmax\\n(QKT\\nâˆšdk\\n)\\nV (3.1)\\nCette architecture permet de parallÃ©liser les calculs, contrairement aux rÃ©seaux rÃ©cur-\\nrents (RNN) prÃ©cÃ©dents', metadata={'source': 'embedding_course_by_koulou.pdf'}),\n",
       "  'original_index': 4,\n",
       "  'score': None}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reranked_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e2cfb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RÃ©sultats RerankÃ©s ===\n",
      "\n",
      "--- RÃ©sultat 1 --- (Score: N/A)\n",
      ".\n",
      "8\n",
      "\n",
      "[PAGE 10]\n",
      "\n",
      "Ã‰tat de lâ€™art Rapport IA 2024\n",
      "4.3.1 Comparatif Technique\n",
      "â€” FAISS (Meta): BibliothÃ¨que bas niveau, optimisÃ©e pour la recherche par simila-\n",
      "ritÃ© dense et le clustering. Utilise lâ€™indexation IVF.\n",
      "â€” ChromaDB : Solution open-source conviviale pour les dÃ©veloppeurs, idÃ©ale pour\n",
      "les applications LLM lÃ©gÃ¨res et le prototypage.\n",
      "â€” Pinecone / Qdrant: Solutions gÃ©rÃ©es (SaaS) ou robustes (Rust) pour la produc-\n",
      "tion Ã  grande Ã©chelle.\n",
      "4.3.2 Exemple dâ€™implÃ©mentation (Python)\n",
      "Voici comment on ini ...\n",
      "--- RÃ©sultat 2 --- (Score: N/A)\n",
      ". Dans ce contexte, les rÃ©seaux de neurones profonds et les\n",
      "bases de donnÃ©es vectorielles jouent un rÃ´le crucial pour traiter des informations non\n",
      "structurÃ©es. Ce paragraphe sert de remplissage pour simuler le volume de texte du rap-\n",
      "port ï¬nal, permettant dâ€™apprÃ©cier la mise en page sans Ãªtre distrait par du latin. Les\n",
      "avancÃ©es technologiques rÃ©centes, notamment les architectures Transformers, ont ouvert\n",
      "la voie aux modÃ¨les de langage massifs (LLM).\n",
      "4.2 Mesure de la SimilaritÃ©\n",
      "Comme vu prÃ©cÃ©demm ...\n",
      "--- RÃ©sultat 3 --- (Score: N/A)\n",
      ". add (\n",
      "9 documents =[\"Lâ€™IA vectorielle est puissante \", \"Le chat mange des\n",
      "croquettes \"],\n",
      "10 metadatas =[{ \" source \": \" cours \"}, {\" source \": \" vie \"}] ,\n",
      "11 ids =[\" id1 \", \" id2 \"]\n",
      "12 )\n",
      "13\n",
      "14 # Recherche sÃ© mantique (Le plus proche voisin )\n",
      "15 results = collection . query (\n",
      "16 query_texts =[\" Base de donn Ã©es vecteur \"],\n",
      "17 n_results =1\n",
      "18 )\n",
      "19\n",
      "Listing 4.1 â€“ Exemple de code ChromaDB pour la recherche sÃ©mantique\n",
      "Lâ€™intelligence artiï¬cielle est un domaine en pleine expansion qui transforme profo ...\n",
      "--- RÃ©sultat 4 --- (Score: N/A)\n",
      ". Ce paragraphe\n",
      "sert de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©-\n",
      "cier la mise en page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques rÃ©centes,\n",
      "notamment les architectures Transformers, ont ouvert la voie aux modÃ¨les de langage\n",
      "massifs (LLM).\n",
      "7\n",
      "\n",
      "[PAGE 9]\n",
      "\n",
      "Chapitre 4\n",
      "Infrastructure de lâ€™IA : Embeddings et\n",
      "Bases Vectorielles\n",
      "Câ€™est ici que rÃ©side le cÅ“ur des systÃ¨mes RAG (GÃ©nÃ©ration AugmentÃ©e par la RÃ©cu-\n",
      "pÃ©ration) modernes.\n",
      "4.1 Le Concept dâ€™Embe ...\n",
      "--- RÃ©sultat 5 --- (Score: N/A)\n",
      ". Les algorithmes dâ€™apprentissage automatique permettent aux ordinateurs\n",
      "dâ€™apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s pour chaque tÃ¢che.\n",
      "Dans ce contexte, les rÃ©seaux de neurones profonds et les bases de donnÃ©es vectorielles\n",
      "jouent un rÃ´le crucial pour traiter des informations non structurÃ©es. Ce paragraphe sert\n",
      "de remplissage pour simuler le volume de texte du rapport ï¬nal, permettant dâ€™apprÃ©cier la\n",
      "mise en page sans Ãªtre distrait par du latin. Les avancÃ©es technologiques  ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"\\n=== RÃ©sultats RerankÃ©s ===\\n\")\n",
    "for i, res in enumerate(reranked_results, 1):\n",
    "    score_str = f\"{res['score']:.4f}\" if res['score'] is not None else \"N/A\"\n",
    "    print(f\"--- RÃ©sultat {i} --- (Score: {score_str})\")\n",
    "    content = getattr(res[\"doc\"], \"page_content\", \"\") or getattr(res[\"doc\"], \"content\", \"\")\n",
    "    print(content[:500], \"...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
